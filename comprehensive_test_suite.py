#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•å¥—ä»¶ - å½»åº•æµ‹è¯•ä¿®æ”¹åçš„è„šæœ¬æ–‡ä»¶å’Œç›¸å…³ä»£ç 
æµ‹è¯•æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å’Œä¿®å¤çš„é—®é¢˜
"""

import os
import sys
import traceback
import tempfile
import shutil
from pathlib import Path
import numpy as np
import pandas as pd
import torch
import yaml
import multiprocessing as mp

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

class ComprehensiveTestSuite:
    """å…¨é¢æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.test_results = {}
        self.temp_dir = None
        self.passed_tests = 0
        self.total_tests = 0
        
    def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("=== è®¾ç½®æµ‹è¯•ç¯å¢ƒ ===")
        
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        self.temp_dir = tempfile.mkdtemp(prefix="factor_test_")
        print(f"ä¸´æ—¶æµ‹è¯•ç›®å½•: {self.temp_dir}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.create_test_data()
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        self.create_test_config()
        
        print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ\n")
    
    def create_test_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶"""
        test_data_dir = Path(self.temp_dir) / "test_data"
        test_data_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„parquetæ–‡ä»¶
        for i in range(3):
            # åˆ›å»ºæ¨¡æ‹Ÿçš„å› å­æ•°æ®
            n_samples = 1000
            n_factors = 100
            
            # ç”Ÿæˆå› å­æ•°æ®
            factor_data = np.random.randn(n_samples, n_factors).astype(np.float32)
            factor_df = pd.DataFrame(factor_data, columns=[str(j) for j in range(n_factors)])
            
            # æ·»åŠ å¿…è¦çš„åˆ—
            factor_df['sid'] = np.random.randint(1, 1000, n_samples)
            factor_df['date'] = pd.date_range(f'2023-{i+1:02d}-01', periods=n_samples, freq='min')
            
            # æ·»åŠ ç›®æ ‡å˜é‡
            factor_df['intra30m'] = np.random.randn(n_samples) * 0.01
            factor_df['nextT1d'] = np.random.randn(n_samples) * 0.02
            factor_df['ema1d'] = np.random.randn(n_samples) * 0.015
            
            # ä¿å­˜ä¸ºparquetæ–‡ä»¶
            file_path = test_data_dir / f"test_data_{i:03d}.parquet"
            factor_df.to_parquet(file_path, index=False)
            
        print(f"âœ… åˆ›å»ºäº†3ä¸ªæµ‹è¯•æ•°æ®æ–‡ä»¶åœ¨ {test_data_dir}")
    
    def create_test_config(self):
        """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
        config = {
            'batch_size': 32,
            'fixed_batch_size': 32,
            'num_workers': 2,
            'epochs': 1,
            'learning_rate': 0.001,
            'input_dim': 100,
            'hidden_dim': 128,
            'num_layers': 2,
            'num_heads': 4,
            'sequence_length': 10,
            'use_distributed': False,
            'use_mixed_precision': False,
            'enable_ic_reporting': False,
            'data_dir': str(Path(self.temp_dir) / "test_data"),
            'output_dir': str(Path(self.temp_dir) / "outputs"),
            'target_columns': ['intra30m', 'nextT1d', 'ema1d'],
            'validation_interval': 1,
            'log_interval': 10,
            'checkpoint_frequency': 1
        }
        
        config_path = Path(self.temp_dir) / "test_config.yaml"
        with open(config_path, 'w') as f:
            yaml.dump(config, f)
        
        self.test_config_path = config_path
        print(f"âœ… æµ‹è¯•é…ç½®æ–‡ä»¶åˆ›å»º: {config_path}")
    
    def test_imports(self):
        """æµ‹è¯•1: æ‰€æœ‰å¯¼å…¥æ˜¯å¦æ­£å¸¸"""
        print("æµ‹è¯•1: å¯¼å…¥æµ‹è¯•")
        try:
            # æµ‹è¯•ä¿®å¤åçš„è„šæœ¬å¯¼å…¥
            from src.unified_complete_training_v2_fixed import (
                UnifiedCompleteTrainer, 
                load_config, 
                main,
                run_worker
            )
            print("  âœ… ä¸»è®­ç»ƒè„šæœ¬å¯¼å…¥æˆåŠŸ")
            
            # æµ‹è¯•æ ¸å¿ƒç»„ä»¶å¯¼å…¥
            from src.models.advanced_tcn_attention import create_advanced_model
            print("  âœ… æ¨¡å‹ç»„ä»¶å¯¼å…¥æˆåŠŸ")
            
            from src.data_processing.optimized_streaming_loader import (
                OptimizedStreamingDataLoader, 
                OptimizedStreamingDataset
            )
            print("  âœ… æ•°æ®åŠ è½½ç»„ä»¶å¯¼å…¥æˆåŠŸ")
            
            from src.training.quantitative_loss import (
                QuantitativeCorrelationLoss, 
                AdaptiveQuantitativeLoss
            )
            print("  âœ… æŸå¤±å‡½æ•°ç»„ä»¶å¯¼å…¥æˆåŠŸ")
            
            from src.data_processing.adaptive_memory_manager import create_memory_manager
            print("  âœ… å†…å­˜ç®¡ç†ç»„ä»¶å¯¼å…¥æˆåŠŸ")
            
            return True
            
        except Exception as e:
            print(f"  âŒ å¯¼å…¥å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_config_loading(self):
        """æµ‹è¯•2: é…ç½®åŠ è½½åŠŸèƒ½"""
        print("\næµ‹è¯•2: é…ç½®åŠ è½½æµ‹è¯•")
        try:
            from src.unified_complete_training_v2_fixed import load_config
            
            # æµ‹è¯•å­˜åœ¨çš„é…ç½®æ–‡ä»¶
            config = load_config(str(self.test_config_path))
            print(f"  âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(config)} ä¸ªå‚æ•°")
            
            # æµ‹è¯•ä¸å­˜åœ¨çš„é…ç½®æ–‡ä»¶ï¼ˆåº”è¯¥è¿”å›é»˜è®¤é…ç½®ï¼‰
            default_config = load_config("nonexistent.yaml")
            print(f"  âœ… é»˜è®¤é…ç½®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(default_config)} ä¸ªå‚æ•°")
            
            # éªŒè¯å…³é”®å‚æ•°
            required_keys = ['batch_size', 'num_workers', 'data_dir']
            for key in required_keys:
                if key not in config:
                    print(f"  âŒ ç¼ºå°‘å…³é”®å‚æ•°: {key}")
                    return False
                print(f"  âœ… å…³é”®å‚æ•° {key}: {config[key]}")
            
            return True
            
        except Exception as e:
            print(f"  âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_memory_manager(self):
        """æµ‹è¯•3: å†…å­˜ç®¡ç†å™¨"""
        print("\næµ‹è¯•3: å†…å­˜ç®¡ç†å™¨æµ‹è¯•")
        try:
            from src.data_processing.adaptive_memory_manager import create_memory_manager
            
            # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
            memory_manager = create_memory_manager({
                'critical_threshold': 0.98,
                'warning_threshold': 0.95
            })
            print("  âœ… å†…å­˜ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•å†…å­˜çŠ¶æ€è·å–
            status = memory_manager.get_memory_status()
            print(f"  âœ… ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡: {status['system']['usage_ratio']:.1%}")
            
            # æµ‹è¯•é˜ˆå€¼è®¾ç½®
            if memory_manager.critical_threshold == 0.98:
                print("  âœ… å…³é”®é˜ˆå€¼è®¾ç½®æ­£ç¡®: 98%")
            else:
                print(f"  âŒ å…³é”®é˜ˆå€¼é”™è¯¯: {memory_manager.critical_threshold}")
                return False
                
            if memory_manager.warning_threshold == 0.95:
                print("  âœ… è­¦å‘Šé˜ˆå€¼è®¾ç½®æ­£ç¡®: 95%")
            else:
                print(f"  âŒ è­¦å‘Šé˜ˆå€¼é”™è¯¯: {memory_manager.warning_threshold}")
                return False
            
            return True
            
        except Exception as e:
            print(f"  âŒ å†…å­˜ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_model_creation(self):
        """æµ‹è¯•4: æ¨¡å‹åˆ›å»º"""
        print("\næµ‹è¯•4: æ¨¡å‹åˆ›å»ºæµ‹è¯•")
        try:
            from src.models.advanced_tcn_attention import create_advanced_model
            
            model_config = {
                'input_dim': 100,
                'hidden_dim': 128,
                'num_layers': 2,
                'num_heads': 4,
                'dropout_rate': 0.1,
                'attention_dropout': 0.1,
                'sequence_length': 10,
                'num_targets': 3,
                'num_stocks': 1000
            }
            
            model = create_advanced_model(model_config)
            print("  âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•æ¨¡å‹å‚æ•°
            total_params = sum(p.numel() for p in model.parameters())
            print(f"  âœ… æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            batch_size = 4
            seq_len = 10
            input_dim = 100
            
            features = torch.randn(batch_size, seq_len, input_dim)
            stock_ids = torch.randint(0, 1000, (batch_size, 1))
            
            with torch.no_grad():
                output = model(features, stock_ids)
                if isinstance(output, dict):
                    print(f"  âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå­—å…¸åŒ…å«: {list(output.keys())}")
                    for key, value in output.items():
                        print(f"    {key}: {value.shape}")
                else:
                    print(f"  âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
            
            return True
            
        except Exception as e:
            print(f"  âŒ æ¨¡å‹åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_loss_functions(self):
        """æµ‹è¯•5: æŸå¤±å‡½æ•°"""
        print("\næµ‹è¯•5: æŸå¤±å‡½æ•°æµ‹è¯•")
        try:
            from src.training.quantitative_loss import (
                QuantitativeCorrelationLoss, 
                AdaptiveQuantitativeLoss
            )
            
            # æµ‹è¯•é‡åŒ–ç›¸å…³æŸå¤±
            loss_fn = QuantitativeCorrelationLoss()
            print("  âœ… QuantitativeCorrelationLossåˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•æŸå¤±è®¡ç®—
            batch_size = 8
            target_names = ['intra30m', 'nextT1d', 'ema1d']
            
            # åˆ›å»ºå­—å…¸æ ¼å¼çš„é¢„æµ‹å’Œç›®æ ‡
            predictions = {}
            targets = {}
            for i, name in enumerate(target_names):
                predictions[name] = torch.randn(batch_size, requires_grad=True)
                targets[name] = torch.randn(batch_size)
            
            loss = loss_fn(predictions, targets)
            if isinstance(loss, torch.Tensor):
                print(f"  âœ… æŸå¤±è®¡ç®—æˆåŠŸï¼ŒæŸå¤±å€¼: {loss.item():.6f}")
            else:
                print(f"  âœ… æŸå¤±è®¡ç®—æˆåŠŸï¼ŒæŸå¤±å€¼: {loss:.6f}")
            
            # æµ‹è¯•è‡ªé€‚åº”æŸå¤±
            adaptive_loss_fn = AdaptiveQuantitativeLoss()
            adaptive_loss = adaptive_loss_fn(predictions, targets)
            print(f"  âœ… è‡ªé€‚åº”æŸå¤±è®¡ç®—æˆåŠŸï¼ŒæŸå¤±å€¼: {adaptive_loss.item():.6f}")
            
            return True
            
        except Exception as e:
            print(f"  âŒ æŸå¤±å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_data_loader(self):
        """æµ‹è¯•6: æ•°æ®åŠ è½½å™¨"""
        print("\næµ‹è¯•6: æ•°æ®åŠ è½½å™¨æµ‹è¯•")
        try:
            from src.data_processing.optimized_streaming_loader import (
                OptimizedStreamingDataLoader,
                OptimizedStreamingDataset
            )
            from src.data_processing.adaptive_memory_manager import create_memory_manager
            
            # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
            memory_manager = create_memory_manager()
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            data_loader = OptimizedStreamingDataLoader(
                data_dir=str(Path(self.temp_dir) / "test_data"),
                memory_manager=memory_manager,
                max_workers=2,
                enable_async_loading=True
            )
            print("  âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
            
            # åˆ›å»ºæ•°æ®é›†
            factor_columns = [str(i) for i in range(100)]
            target_columns = ['intra30m', 'nextT1d', 'ema1d']
            
            dataset = OptimizedStreamingDataset(
                data_loader=data_loader,
                factor_columns=factor_columns,
                target_columns=target_columns,
                sequence_length=10,
                start_date=None,
                end_date=None
            )
            print("  âœ… æµå¼æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•æ•°æ®è¿­ä»£
            sample_count = 0
            for sample in dataset:
                if sample_count >= 3:  # åªæµ‹è¯•å‰å‡ ä¸ªæ ·æœ¬
                    break
                
                print(f"  âœ… æ ·æœ¬ {sample_count + 1}:")
                print(f"    ç‰¹å¾å½¢çŠ¶: {sample['features'].shape}")
                print(f"    ç›®æ ‡å½¢çŠ¶: {sample['targets'].shape}")
                print(f"    è‚¡ç¥¨IDå½¢çŠ¶: {sample['stock_id'].shape}")
                
                sample_count += 1
            
            if sample_count > 0:
                print(f"  âœ… æˆåŠŸè¯»å– {sample_count} ä¸ªæ ·æœ¬")
                return True
            else:
                print("  âŒ æœªèƒ½è¯»å–ä»»ä½•æ ·æœ¬")
                return False
            
        except Exception as e:
            print(f"  âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_trainer_initialization(self):
        """æµ‹è¯•7: è®­ç»ƒå™¨åˆå§‹åŒ–"""
        print("\næµ‹è¯•7: è®­ç»ƒå™¨åˆå§‹åŒ–æµ‹è¯•")
        try:
            from src.unified_complete_training_v2_fixed import (
                UnifiedCompleteTrainer, 
                load_config
            )
            
            # åŠ è½½æµ‹è¯•é…ç½®
            config = load_config(str(self.test_config_path))
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = UnifiedCompleteTrainer(config, rank=0, world_size=1)
            print("  âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
            print(f"  âœ… è®¾å¤‡: {trainer.device}")
            
            # æµ‹è¯•åˆ†å¸ƒå¼è®¾ç½®
            trainer.setup_distributed()
            print("  âœ… åˆ†å¸ƒå¼è®¾ç½®å®Œæˆ")
            
            # æµ‹è¯•æ•°æ®åŠ è½½å™¨è®¾ç½®
            trainer.setup_data_loaders()
            print("  âœ… æ•°æ®åŠ è½½å™¨è®¾ç½®å®Œæˆ")
            
            # æµ‹è¯•æ¨¡å‹åˆ›å»º
            trainer.create_model()
            print("  âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
            
            # éªŒè¯ç»„ä»¶
            if trainer.model is not None:
                print("  âœ… æ¨¡å‹å·²åˆ›å»º")
            if trainer.optimizer is not None:
                print("  âœ… ä¼˜åŒ–å™¨å·²åˆ›å»º")
            if trainer.criterion is not None:
                print("  âœ… æŸå¤±å‡½æ•°å·²åˆ›å»º")
            
            return True
            
        except Exception as e:
            print(f"  âŒ è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_multiprocessing_compatibility(self):
        """æµ‹è¯•8: å¤šè¿›ç¨‹å…¼å®¹æ€§"""
        print("\næµ‹è¯•8: å¤šè¿›ç¨‹å…¼å®¹æ€§æµ‹è¯•")
        try:
            # æµ‹è¯•multiprocessingè®¾ç½®
            original_method = mp.get_start_method(allow_none=True)
            print(f"  å½“å‰multiprocessingæ–¹æ³•: {original_method}")
            
            # æµ‹è¯•spawnæ–¹æ³•è®¾ç½®
            try:
                mp.set_start_method('spawn', force=True)
                print("  âœ… spawnæ–¹æ³•è®¾ç½®æˆåŠŸ")
            except RuntimeError as e:
                if "context has already been set" in str(e):
                    print("  âœ… spawnæ–¹æ³•å·²è®¾ç½®")
                else:
                    raise
            
            # æµ‹è¯•ç®€å•çš„å¤šè¿›ç¨‹ä»»åŠ¡ï¼ˆä¸å®é™…å¯åŠ¨ï¼‰
            def simple_worker(x):
                return x * 2
            
            # åªæµ‹è¯•å‡½æ•°å®šä¹‰ï¼Œä¸å®é™…è¿è¡Œå¤šè¿›ç¨‹
            print("  âœ… å¤šè¿›ç¨‹workerå‡½æ•°å®šä¹‰æ­£å¸¸")
            
            return True
            
        except Exception as e:
            print(f"  âŒ å¤šè¿›ç¨‹å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def test_syntax_and_imports_all_files(self):
        """æµ‹è¯•9: æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„è¯­æ³•å’Œå¯¼å…¥"""
        print("\næµ‹è¯•9: æ‰€æœ‰ç›¸å…³æ–‡ä»¶è¯­æ³•æ£€æŸ¥")
        
        test_files = [
            "src/unified_complete_training_v2_fixed.py",
            "src/data_processing/adaptive_memory_manager.py",
            "src/models/advanced_tcn_attention.py",
            "src/training/quantitative_loss.py",
            "src/data_processing/optimized_streaming_loader.py"
        ]
        
        all_passed = True
        
        for file_path in test_files:
            if Path(file_path).exists():
                try:
                    # è¯­æ³•æ£€æŸ¥
                    import ast
                    with open(file_path, 'r', encoding='utf-8') as f:
                        ast.parse(f.read())
                    print(f"  âœ… {file_path} è¯­æ³•æ£€æŸ¥é€šè¿‡")
                    
                except SyntaxError as e:
                    print(f"  âŒ {file_path} è¯­æ³•é”™è¯¯: {e}")
                    all_passed = False
                    
                except Exception as e:
                    print(f"  âŒ {file_path} æ£€æŸ¥å¤±è´¥: {e}")
                    all_passed = False
            else:
                print(f"  âš ï¸  {file_path} æ–‡ä»¶ä¸å­˜åœ¨")
        
        return all_passed
    
    def test_pickle_serialization(self):
        """æµ‹è¯•10: pickleåºåˆ—åŒ–å…¼å®¹æ€§"""
        print("\næµ‹è¯•10: pickleåºåˆ—åŒ–æµ‹è¯•")
        try:
            import pickle
            from src.unified_complete_training_v2_fixed import load_config
            
            # æµ‹è¯•é…ç½®å¯¹è±¡åºåˆ—åŒ–
            config = load_config(str(self.test_config_path))
            
            # å°è¯•åºåˆ—åŒ–é…ç½®
            serialized = pickle.dumps(config)
            deserialized = pickle.loads(serialized)
            print("  âœ… é…ç½®å¯¹è±¡åºåˆ—åŒ–æˆåŠŸ")
            
            # æµ‹è¯•ç®€å•å¯¹è±¡åºåˆ—åŒ–
            test_data = {
                'numbers': [1, 2, 3],
                'text': 'test',
                'nested': {'key': 'value'}
            }
            
            serialized_data = pickle.dumps(test_data)
            deserialized_data = pickle.loads(serialized_data)
            print("  âœ… åŸºæœ¬æ•°æ®ç»“æ„åºåˆ—åŒ–æˆåŠŸ")
            
            return True
            
        except Exception as e:
            print(f"  âŒ pickleåºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def run_test(self, test_func):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        self.total_tests += 1
        try:
            if test_func():
                self.passed_tests += 1
                self.test_results[test_func.__name__] = "PASSED"
                return True
            else:
                self.test_results[test_func.__name__] = "FAILED"
                return False
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            self.test_results[test_func.__name__] = f"ERROR: {e}"
            return False
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å…¨é¢æµ‹è¯•ä¿®æ”¹åçš„è„šæœ¬æ–‡ä»¶å’Œç›¸å…³ä»£ç \n")
        
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        self.setup_test_environment()
        
        # å®šä¹‰æ‰€æœ‰æµ‹è¯•
        tests = [
            self.test_imports,
            self.test_config_loading,
            self.test_memory_manager,
            self.test_model_creation,
            self.test_loss_functions,
            self.test_data_loader,
            self.test_trainer_initialization,
            self.test_multiprocessing_compatibility,
            self.test_syntax_and_imports_all_files,
            self.test_pickle_serialization
        ]
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for test_func in tests:
            self.run_test(test_func)
            print()  # æ·»åŠ ç©ºè¡Œåˆ†éš”
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("=" * 60)
        print("ğŸ¯ å…¨é¢æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        print(f"æ€»æµ‹è¯•æ•°: {self.total_tests}")
        print(f"é€šè¿‡æµ‹è¯•: {self.passed_tests}")
        print(f"å¤±è´¥æµ‹è¯•: {self.total_tests - self.passed_tests}")
        print(f"é€šè¿‡ç‡: {self.passed_tests / self.total_tests * 100:.1f}%")
        print()
        
        # è¯¦ç»†ç»“æœ
        print("è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in self.test_results.items():
            status_icon = "âœ…" if result == "PASSED" else "âŒ"
            print(f"  {status_icon} {test_name}: {result}")
        
        print()
        
        # æ€»ç»“
        if self.passed_tests == self.total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®æ”¹åçš„è„šæœ¬å®Œå…¨æ­£å¸¸ã€‚")
            print("\nâœ… ä¿®å¤éªŒè¯:")
            print("  - pickleåºåˆ—åŒ–é—®é¢˜: å·²ä¿®å¤")
            print("  - ä»£ç ç¼©è¿›é”™è¯¯: å·²ä¿®å¤")
            print("  - å†…å­˜ç®¡ç†å™¨ä¼˜åŒ–: å·²éªŒè¯")
            print("  - æ•°æ®åŠ è½½å™¨ä¼˜åŒ–: å·²éªŒè¯")
            print("  - å¤šè¿›ç¨‹å…¼å®¹æ€§: å·²éªŒè¯")
            print("  - æ‰€æœ‰ç»„ä»¶é›†æˆ: å·²éªŒè¯")
            
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
            
            failed_tests = [name for name, result in self.test_results.items() if result != "PASSED"]
            print(f"\nå¤±è´¥çš„æµ‹è¯•: {failed_tests}")
        
        print("\n" + "=" * 60)
        
        return self.passed_tests == self.total_tests
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.temp_dir and Path(self.temp_dir).exists():
            shutil.rmtree(self.temp_dir)
            print(f"âœ… æ¸…ç†æµ‹è¯•ç›®å½•: {self.temp_dir}")


def main():
    """ä¸»å‡½æ•°"""
    test_suite = ComprehensiveTestSuite()
    
    try:
        success = test_suite.run_all_tests()
        return 0 if success else 1
    finally:
        test_suite.cleanup()


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
