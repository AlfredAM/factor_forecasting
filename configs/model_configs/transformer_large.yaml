# Large Transformer Model Configuration for Factor Forecasting
# This configuration prioritizes accuracy over computational efficiency

model:
  name: "transformer_large"
  type: "transformer"
  
  # Architecture parameters (larger than base)
  hidden_size: 512
  num_layers: 12
  num_heads: 16
  dropout: 0.15
  activation: "gelu"
  d_ff: 2048
  max_seq_len: 100
  
  # Input/output dimensions
  num_factors: 100
  num_targets: 3
  embedding_dim: 512
  
  # Attention configuration
  use_flash_attention: true
  attention_dropout: 0.15
  layer_norm_eps: 1e-6

training:
  # Training parameters (adjusted for larger model)
  batch_size: 32  # Smaller batch size due to larger model
  learning_rate: 5e-5  # Lower learning rate for stability
  weight_decay: 1e-4
  num_epochs: 150
  gradient_clip: 0.5
  
  # Learning rate scheduling
  scheduler_type: "cosine"
  warmup_steps: 2000
  min_lr: 5e-7
  
  # Early stopping
  early_stopping_patience: 15
  early_stopping_min_delta: 0.0005
  restore_best_weights: true
  
  # Mixed precision
  use_mixed_precision: true
  gradient_accumulation_steps: 2

data:
  # Sequence configuration (longer sequences)
  sequence_length: 50
  prediction_horizon: 1
  min_sequence_length: 20
  
  # Target columns
  target_columns: ["intra30m", "nextT1d", "ema1d"]
  factor_columns: null
  stock_id_column: "sid"
  weight_column: "ADV50"
  
  # Data split ratios
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

loss:
  # Loss function configuration
  type: "correlation_loss"
  correlation_weight: 1.0
  mse_weight: 0.05
  rank_correlation_weight: 0.2
  
  # Target correlations
  target_correlations: [0.12, 0.06, 0.1]
  
  # Quantile regression
  quantile_alpha: 0.5

optimization:
  # Optimizer configuration
  optimizer_type: "adamw"
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  
  # Regularization
  label_smoothing: 0.05
  dropout: 0.15

data_augmentation:
  enabled: true
  noise_std: 0.005
  mask_probability: 0.15
  random_rotation: false

output:
  model_save_dir: "models/transformer_large"
  best_model_name: "best_model.pth"
  checkpoint_interval: 5
  
  # Logging
  log_dir: "logs/transformer_large"
  use_wandb: true
  wandb_project: "factor_forecasting"
  wandb_run_name: "transformer_large"

hardware:
  device: "auto"
  num_workers: 4
  pin_memory: true

# Training mode specific settings
training_mode: "rolling_window"
rolling_window_years: 2  # Longer training window
min_train_years: 2
prediction_years: [2019, 2020, 2021, 2022] 