# 保守的4GPU分布式训练配置 - 避免内存问题
model_type: AdvancedFactorForecastingTCNAttention
input_dim: 100
hidden_dim: 512    # 减小模型大小
num_layers: 8      # 减少层数
num_heads: 8       # 减少注意力头数
tcn_kernel_size: 5
tcn_dilation_factor: 2
dropout_rate: 0.1
attention_dropout: 0.05
target_columns: [intra30m, nextT1d, ema1d]
sequence_length: 32  # 减小序列长度
epochs: 200
batch_size: 2048     # 4GPU总批次大小 (每GPU 512)
fixed_batch_size: 2048
learning_rate: 0.0002
weight_decay: 0.01
gradient_clip_norm: 1.0
use_mixed_precision: true
accumulation_steps: 1
use_adaptive_batch_size: false
num_workers: 0
pin_memory: false
use_distributed: true
auto_resume: true
log_level: INFO
ic_report_interval: 7200
enable_ic_reporting: true
checkpoint_frequency: 5
save_all_checkpoints: false
output_dir: /nas/factor_forecasting/outputs
data_dir: /nas/feature_v2_10s
train_start_date: 2018-01-02
train_end_date: 2018-10-31   # 前10个月数据
val_start_date: 2018-11-01
val_end_date: 2018-11-30
test_start_date: 2018-12-01
test_end_date: 2018-12-31
enforce_next_year_prediction: false
enable_yearly_rolling: false
min_train_years: 1
rolling_window_years: 1
shuffle_buffer_size: 512

# 分布式训练专用参数
world_size: 4
backend: nccl
init_method: env://

# 内存优化
enable_gradient_checkpointing: true
max_memory_per_gpu: 18  # 保守的内存限制
