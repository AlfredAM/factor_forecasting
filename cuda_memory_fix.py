#!/usr/bin/env python3
"""
CUDAå†…å­˜ç¢ç‰‡åŒ–é—®é¢˜å½»åº•è§£å†³æ–¹æ¡ˆ
ä»æ ¹æœ¬ä¸Šè§£å†³PyTorch CUDAå†…å­˜ç®¡ç†é—®é¢˜
"""

import subprocess
import os

def create_memory_optimized_config():
    """åˆ›å»ºå†…å­˜ä¼˜åŒ–é…ç½®"""
    config_content = """# å†…å­˜ä¼˜åŒ–4GPUé…ç½® - å½»åº•è§£å†³CUDA OOMé—®é¢˜
model_type: AdvancedFactorForecastingTCNAttention
input_dim: 100
hidden_dim: 768       # é€‚ä¸­çš„éšè—ç»´åº¦
num_layers: 8         # å‡å°‘å±‚æ•°é¿å…å†…å­˜çˆ†ç‚¸
num_heads: 12         # é€‚ä¸­çš„æ³¨æ„åŠ›å¤´æ•°
tcn_kernel_size: 5    # å‡å°å·ç§¯æ ¸
tcn_dilation_factor: 2
dropout_rate: 0.1
attention_dropout: 0.05
target_columns: [intra30m, nextT1d, ema1d]
sequence_length: 48   # å‡å°åºåˆ—é•¿åº¦
epochs: 200
batch_size: 1024      # æ¯GPUåˆç†æ‰¹æ¬¡å¤§å°
fixed_batch_size: 1024
learning_rate: 0.0001
weight_decay: 0.01
gradient_clip_norm: 1.0
use_mixed_precision: true
accumulation_steps: 2  # æ¢¯åº¦ç´¯ç§¯
use_adaptive_batch_size: true  # å¯ç”¨è‡ªé€‚åº”æ‰¹æ¬¡
adaptive_batch_size: true
num_workers: 0
pin_memory: false     # å…³é—­pin_memoryå‡å°‘å†…å­˜ä½¿ç”¨
use_distributed: true
world_size: 4
backend: nccl
auto_resume: true
log_level: INFO
ic_report_interval: 7200
enable_ic_reporting: true
checkpoint_frequency: 10
save_all_checkpoints: false
output_dir: /nas/factor_forecasting/outputs
data_dir: /nas/feature_v2_10s
train_start_date: 2018-01-02
train_end_date: 2018-06-30   # å‡å°‘æ•°æ®é‡
val_start_date: 2018-07-01
val_end_date: 2018-08-31
test_start_date: 2018-09-01
test_end_date: 2018-12-31
enforce_next_year_prediction: false
enable_yearly_rolling: false
min_train_years: 1
rolling_window_years: 1
shuffle_buffer_size: 512

# CUDAå†…å­˜ä¼˜åŒ–è®¾ç½®
cuda_memory_fraction: 0.85    # é™åˆ¶æ¯GPUå†…å­˜ä½¿ç”¨
enable_memory_pool: true      # å¯ç”¨å†…å­˜æ± 
memory_pool_init_size: 1024   # åˆå§‹å†…å­˜æ± å¤§å°MB
memory_pool_max_size: 20480   # æœ€å¤§å†…å­˜æ± å¤§å°MB
enable_garbage_collection: true  # å¯ç”¨åƒåœ¾å›æ”¶
gc_frequency: 50              # æ¯50ä¸ªbatchæ¸…ç†ä¸€æ¬¡
"""
    return config_content

def create_cuda_memory_patch():
    """åˆ›å»ºCUDAå†…å­˜ç®¡ç†è¡¥ä¸"""
    patch_content = '''#!/usr/bin/env python3
"""
CUDAå†…å­˜ç®¡ç†è¡¥ä¸
å½»åº•è§£å†³å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜
"""

import torch
import torch.cuda
import gc
import os
import logging

class CUDAMemoryManager:
    def __init__(self):
        self.setup_cuda_environment()
        self.setup_memory_pool()
        
    def setup_cuda_environment(self):
        """è®¾ç½®CUDAç¯å¢ƒå˜é‡"""
        # å…³é”®ç¯å¢ƒå˜é‡è®¾ç½®
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = (
            'expandable_segments:True,'
            'max_split_size_mb:128,'
            'roundup_power2_divisions:16,'
            'garbage_collection_threshold:0.8'
        )
        
        # è®¾ç½®CUDAè®¾å¤‡
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.85)  # é™åˆ¶å†…å­˜ä½¿ç”¨
            torch.backends.cudnn.benchmark = False  # ç¦ç”¨cudnn benchmarkå‡å°‘å†…å­˜
            torch.backends.cudnn.deterministic = True
            
    def setup_memory_pool(self):
        """è®¾ç½®å†…å­˜æ± """
        if torch.cuda.is_available():
            # æ¸…ç©ºç¼“å­˜
            torch.cuda.empty_cache()
            
            # è®¾ç½®å†…å­˜æ± 
            torch.cuda.set_memory_strategy('expandable_segments')
            
            logging.info("CUDAå†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
    def cleanup_memory(self, aggressive=False):
        """æ¸…ç†å†…å­˜"""
        if torch.cuda.is_available():
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()
            
            if aggressive:
                # æ¿€è¿›æ¸…ç†æ¨¡å¼
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
                gc.collect()
                
            # è®°å½•å†…å­˜çŠ¶æ€
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                logging.info(f"GPU {i}: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB")
                
    def get_memory_info(self, device_id=0):
        """è·å–å†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(device_id) / 1024**3
            reserved = torch.cuda.memory_reserved(device_id) / 1024**3
            max_allocated = torch.cuda.max_memory_allocated(device_id) / 1024**3
            
            return {
                'allocated': allocated,
                'reserved': reserved,
                'max_allocated': max_allocated,
                'free': (torch.cuda.get_device_properties(device_id).total_memory / 1024**3) - reserved
            }
        return None

# å…¨å±€å†…å­˜ç®¡ç†å™¨
memory_manager = CUDAMemoryManager()

def patch_training_script():
    """ä¿®è¡¥è®­ç»ƒè„šæœ¬"""
    script_path = "/nas/factor_forecasting/unified_complete_training_v2_fixed.py"
    
    # è¯»å–åŸæ–‡ä»¶
    with open(script_path, 'r') as f:
        content = f.read()
    
    # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å†…å­˜ç®¡ç†
    memory_imports = """
# CUDAå†…å­˜ç®¡ç†è¡¥ä¸
import torch
import torch.cuda
import gc
import os

# è®¾ç½®CUDAç¯å¢ƒ
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = (
    'expandable_segments:True,'
    'max_split_size_mb:128,'
    'roundup_power2_divisions:16,'
    'garbage_collection_threshold:0.8'
)

if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.85)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def cleanup_cuda_memory():
    \"\"\"æ¸…ç†CUDAå†…å­˜\"\"\"
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

"""
    
    # å¦‚æœè¿˜æ²¡æœ‰æ·»åŠ å†…å­˜ç®¡ç†ä»£ç 
    if "cleanup_cuda_memory" not in content:
        # æ‰¾åˆ°importséƒ¨åˆ†å¹¶æ·»åŠ 
        import_pos = content.find("import torch")
        if import_pos != -1:
            content = content[:import_pos] + memory_imports + content[import_pos:]
        
        # åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å†…å­˜æ¸…ç†
        # æŸ¥æ‰¾è®­ç»ƒå¾ªç¯
        training_patterns = [
            "for batch_idx, batch in enumerate(",
            "for i, batch in enumerate(",
            "for step, batch in enumerate("
        ]
        
        for pattern in training_patterns:
            if pattern in content:
                # åœ¨æ¯ä¸ªbatchåæ·»åŠ å†…å­˜æ¸…ç†
                lines = content.split('\n')
                new_lines = []
                
                for i, line in enumerate(lines):
                    new_lines.append(line)
                    
                    # åœ¨batchå¤„ç†åæ·»åŠ å†…å­˜æ¸…ç†
                    if "loss.backward()" in line:
                        new_lines.append("                    # å†…å­˜æ¸…ç†")
                        new_lines.append("                    if batch_idx % 10 == 0:")
                        new_lines.append("                        cleanup_cuda_memory()")
                
                content = '\n'.join(new_lines)
                break
    
    # å†™å›æ–‡ä»¶
    with open(script_path, 'w') as f:
        f.write(content)
    
    print("âœ… è®­ç»ƒè„šæœ¬å·²æ·»åŠ CUDAå†…å­˜ç®¡ç†è¡¥ä¸")

def apply_memory_fixes():
    """åº”ç”¨æ‰€æœ‰å†…å­˜ä¿®å¤"""
    print("ğŸ”§ åº”ç”¨CUDAå†…å­˜ä¿®å¤...")
    
    # 1. åˆ›å»ºå†…å­˜ä¼˜åŒ–é…ç½®
    config = create_memory_optimized_config()
    with open("/nas/factor_forecasting/memory_optimized_config.yaml", "w") as f:
        f.write(config)
    print("âœ… åˆ›å»ºå†…å­˜ä¼˜åŒ–é…ç½®æ–‡ä»¶")
    
    # 2. ä¿®è¡¥è®­ç»ƒè„šæœ¬
    patch_training_script()
    
    # 3. åˆ›å»ºå¯åŠ¨è„šæœ¬
    launch_script = """#!/bin/bash
# CUDAå†…å­˜ä¼˜åŒ–å¯åŠ¨è„šæœ¬

set -e

echo "ğŸš€ å¯åŠ¨å†…å­˜ä¼˜åŒ–çš„4GPUè®­ç»ƒ..."

cd /nas/factor_forecasting
source venv/bin/activate

# è®¾ç½®CUDAç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:128,roundup_power2_divisions:16,garbage_collection_threshold:0.8"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_USE_CUDA_DSA=1

# æ¸…ç†æ—§è¿›ç¨‹
pkill -f "torchrun.*unified_complete" 2>/dev/null || true
sleep 5

# æ¸…ç†GPUå†…å­˜
nvidia-smi --gpu-reset || true
sleep 2

echo "ğŸ” æ£€æŸ¥GPUçŠ¶æ€..."
nvidia-smi

echo "ğŸš€ å¯åŠ¨ä¼˜åŒ–è®­ç»ƒ..."
nohup torchrun --standalone --nproc_per_node=4 \\
    unified_complete_training_v2_fixed.py \\
    --config memory_optimized_config.yaml \\
    > training_memory_optimized.log 2>&1 &

TRAIN_PID=$!
echo "è®­ç»ƒå·²å¯åŠ¨ï¼ŒPID: $TRAIN_PID"

# ç­‰å¾…å¯åŠ¨
sleep 10

echo "æ£€æŸ¥è®­ç»ƒçŠ¶æ€..."
ps aux | grep unified_complete | grep -v grep || echo "è®­ç»ƒè¿›ç¨‹æœªæ‰¾åˆ°"

echo "æ£€æŸ¥GPUä½¿ç”¨..."
nvidia-smi

echo "âœ… å†…å­˜ä¼˜åŒ–è®­ç»ƒå¯åŠ¨å®Œæˆ"
"""
    
    with open("/nas/factor_forecasting/launch_memory_optimized.sh", "w") as f:
        f.write(launch_script)
    
    print("âœ… åˆ›å»ºå†…å­˜ä¼˜åŒ–å¯åŠ¨è„šæœ¬")
    print("ğŸ‰ CUDAå†…å­˜ä¿®å¤å®Œæˆ!")

if __name__ == "__main__":
    apply_memory_fixes()
'''
    return patch_content

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ åˆ›å»ºCUDAå†…å­˜ä¿®å¤è„šæœ¬...")
    
    # åˆ›å»ºé…ç½®
    config = create_memory_optimized_config()
    print("âœ… å†…å­˜ä¼˜åŒ–é…ç½®åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºè¡¥ä¸
    patch = create_cuda_memory_patch()
    print("âœ… CUDAå†…å­˜è¡¥ä¸åˆ›å»ºå®Œæˆ")
    
    return config, patch

if __name__ == "__main__":
    main()
