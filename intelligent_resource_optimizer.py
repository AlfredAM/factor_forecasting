#!/usr/bin/env python3
"""
æ™ºèƒ½èµ„æºä¼˜åŒ–å™¨ - æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡åŒæ—¶é¿å…OOM
åŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°ä»¥å……åˆ†åˆ©ç”¨4å¼ A10 GPU + 739GB RAM + 128æ ¸CPU
"""

import subprocess
import time
import json
import re
from datetime import datetime, timedelta
from pathlib import Path

class IntelligentResourceOptimizer:
    def __init__(self):
        self.server_ip = "8.216.35.79"
        self.password = "Abab1234"
        self.project_path = "/nas/factor_forecasting"
        
        # ç¡¬ä»¶è§„æ ¼
        self.total_gpus = 4
        self.gpu_memory_mb = 23028
        self.total_ram_gb = 739
        self.cpu_cores = 128
        
        # å®‰å…¨è¾¹ç•Œ
        self.gpu_memory_safety_margin = 0.05  # 5%å®‰å…¨è¾¹ç•Œ
        self.ram_safety_margin = 0.1  # 10%å®‰å…¨è¾¹ç•Œ
        
        # ç›‘æ§å†å²
        self.performance_history = []
        self.correlation_reports = []
        
    def ssh_execute(self, command):
        """æ‰§è¡ŒSSHå‘½ä»¤"""
        try:
            result = subprocess.run([
                'sshpass', '-p', self.password, 'ssh', '-o', 'StrictHostKeyChecking=no',
                f'ecs-user@{self.server_ip}', command
            ], capture_output=True, text=True, timeout=30)
            return result.returncode == 0, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return False, "", "Command timeout"
        except Exception as e:
            return False, "", str(e)
    
    def get_system_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        status = {
            'timestamp': datetime.now(),
            'gpu_status': {},
            'memory_status': {},
            'process_status': {},
            'training_metrics': {}
        }
        
        # GPUçŠ¶æ€
        success, gpu_output, _ = self.ssh_execute(
            "nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu --format=csv,noheader,nounits"
        )
        
        if success and gpu_output.strip():
            for line in gpu_output.strip().split('\n'):
                parts = line.split(', ')
                if len(parts) >= 6:
                    gpu_id, name, mem_used, mem_total, util, temp = parts
                    status['gpu_status'][int(gpu_id)] = {
                        'memory_used_mb': int(mem_used),
                        'memory_total_mb': int(mem_total),
                        'utilization_pct': int(util),
                        'temperature_c': int(temp),
                        'memory_usage_pct': int(mem_used) / int(mem_total) * 100
                    }
        
        # å†…å­˜çŠ¶æ€
        success, mem_output, _ = self.ssh_execute("free -g | grep Mem:")
        if success and mem_output.strip():
            parts = mem_output.strip().split()
            if len(parts) >= 7:
                total, used, free = int(parts[1]), int(parts[2]), int(parts[3])
                status['memory_status'] = {
                    'total_gb': total,
                    'used_gb': used,
                    'free_gb': free,
                    'usage_pct': used / total * 100
                }
        
        # è®­ç»ƒè¿›ç¨‹çŠ¶æ€
        success, proc_output, _ = self.ssh_execute(
            'ps aux | grep "python.*unified_complete_training" | grep -v grep'
        )
        
        if success and proc_output.strip():
            status['process_status']['training_active'] = True
            # æå–CPUå’Œå†…å­˜ä½¿ç”¨
            for line in proc_output.strip().split('\n'):
                parts = line.split()
                if len(parts) >= 11:
                    status['process_status']['cpu_pct'] = float(parts[2])
                    status['process_status']['memory_pct'] = float(parts[3])
                    status['process_status']['pid'] = int(parts[1])
        else:
            status['process_status']['training_active'] = False
        
        # è®­ç»ƒæŒ‡æ ‡
        success, log_output, _ = self.ssh_execute(
            f"cd {self.project_path} && tail -10 training_memory_optimized.log 2>/dev/null || tail -10 *.log | tail -10"
        )
        
        if success and log_output:
            status['training_metrics'] = self.parse_training_metrics(log_output)
        
        return status
    
    def parse_training_metrics(self, log_output):
        """è§£æè®­ç»ƒæŒ‡æ ‡"""
        metrics = {}
        
        # æŸ¥æ‰¾epochä¿¡æ¯
        epoch_pattern = r'Epoch (\d+) Training: (\d+)it \[([^,]+), ([^]]+)\]'
        matches = re.findall(epoch_pattern, log_output)
        
        if matches:
            epoch, iterations, time_elapsed, time_per_it = matches[-1]
            metrics['current_epoch'] = int(epoch)
            metrics['iterations'] = int(iterations)
            metrics['time_elapsed'] = time_elapsed
            metrics['time_per_iteration'] = time_per_it
            
            # ä¼°ç®—epochå®Œæˆæ—¶é—´
            if 'it/s' in time_per_it:
                its_per_sec = float(time_per_it.split('it/s')[0])
                metrics['iterations_per_second'] = its_per_sec
            elif 's/it' in time_per_it:
                sec_per_it = float(time_per_it.split('s/it')[0])
                metrics['seconds_per_iteration'] = sec_per_it
        
        # æ£€æŸ¥é”™è¯¯
        if 'CUDA out of memory' in log_output:
            metrics['memory_error'] = True
        
        return metrics
    
    def calculate_optimal_config(self, current_status):
        """æ ¹æ®å½“å‰çŠ¶æ€è®¡ç®—æœ€ä¼˜é…ç½®"""
        config = {}
        
        # åˆ†æGPUä½¿ç”¨æƒ…å†µ
        gpu_0_usage = current_status['gpu_status'].get(0, {})
        gpu_memory_used_pct = gpu_0_usage.get('memory_usage_pct', 0)
        
        # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if gpu_memory_used_pct > 95:  # æ¥è¿‘æ»¡è´Ÿè·
            config['batch_size'] = 512  # å‡å°æ‰¹æ¬¡
            config['accumulation_steps'] = 8
        elif gpu_memory_used_pct < 80:  # æœ‰ä½™é‡
            config['batch_size'] = 1024  # å¢å¤§æ‰¹æ¬¡
            config['accumulation_steps'] = 4
        else:
            config['batch_size'] = 768  # ä¸­ç­‰æ‰¹æ¬¡
            config['accumulation_steps'] = 6
        
        # åˆ©ç”¨å¤šGPUï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
        if gpu_memory_used_pct < 85:
            config['use_distributed'] = True
            config['world_size'] = 2  # å…ˆç”¨2ä¸ªGPUæµ‹è¯•
        else:
            config['use_distributed'] = False
        
        # CPUä¼˜åŒ–
        cpu_cores_to_use = min(32, self.cpu_cores // 4)  # ä¿å®ˆä½¿ç”¨CPU
        config['num_workers'] = 0  # ä¿æŒä¸º0é¿å…CUDAé—®é¢˜
        config['dataloader_threads'] = cpu_cores_to_use
        
        # å†…å­˜ä¼˜åŒ–
        ram_usage_pct = current_status['memory_status'].get('usage_pct', 0)
        if ram_usage_pct < 50:  # å†…å­˜å……è¶³
            config['cache_size'] = 50  # å¢å¤§ç¼“å­˜
            config['streaming_chunk_size'] = 50000
        else:
            config['cache_size'] = 20
            config['streaming_chunk_size'] = 10000
        
        return config
    
    def create_optimized_config(self, config_params):
        """åˆ›å»ºä¼˜åŒ–é…ç½®æ–‡ä»¶"""
        config_content = f"""# æ™ºèƒ½ä¼˜åŒ–é…ç½® - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
model_type: AdvancedFactorForecastingTCNAttention
input_dim: 100
hidden_dim: 768    # å¹³è¡¡æ€§èƒ½å’Œå†…å­˜
num_layers: 12     # å……åˆ†åˆ©ç”¨GPUè®¡ç®—èƒ½åŠ›
num_heads: 16      # å……åˆ†çš„æ³¨æ„åŠ›å¤´
tcn_kernel_size: 7
tcn_dilation_factor: 2
dropout_rate: 0.1
attention_dropout: 0.05
target_columns: [intra30m, nextT1d, ema1d]  # æ‰€æœ‰ç›®æ ‡
sequence_length: 60
epochs: 200
batch_size: {config_params['batch_size']}
fixed_batch_size: {config_params['batch_size']}
learning_rate: 0.0001
weight_decay: 0.01
gradient_clip_norm: 1.0
use_mixed_precision: true
accumulation_steps: {config_params['accumulation_steps']}
use_adaptive_batch_size: false
num_workers: 0
pin_memory: false
use_distributed: {str(config_params['use_distributed']).lower()}
auto_resume: true
log_level: INFO
ic_report_interval: 7200
enable_ic_reporting: true
checkpoint_frequency: 5
save_all_checkpoints: false
output_dir: /nas/factor_forecasting/outputs
data_dir: /nas/feature_v2_10s
train_start_date: 2018-01-02
train_end_date: 2018-10-31
val_start_date: 2018-11-01
val_end_date: 2018-12-31
test_start_date: 2019-01-01
test_end_date: 2019-12-31
enforce_next_year_prediction: true
enable_yearly_rolling: true
min_train_years: 1
rolling_window_years: 1
shuffle_buffer_size: {config_params.get('cache_size', 20) * 10}

# æ€§èƒ½ä¼˜åŒ–
streaming_chunk_size: {config_params.get('streaming_chunk_size', 10000)}
cache_size: {config_params.get('cache_size', 20)}
max_memory_usage: {int(self.total_ram_gb * 0.8)}
enable_memory_mapping: true
torch_compile: false  # é¿å…ç¼–è¯‘å¼€é”€
"""
        
        return config_content
    
    def restart_training_if_needed(self, current_status):
        """å¦‚æœéœ€è¦åˆ™é‡å¯è®­ç»ƒ"""
        if not current_status['process_status'].get('training_active', False):
            print("ğŸ”„ è®­ç»ƒè¿›ç¨‹æœªè¿è¡Œï¼Œå‡†å¤‡é‡æ–°å¯åŠ¨...")
            
            # è®¡ç®—æœ€ä¼˜é…ç½®
            optimal_config = self.calculate_optimal_config(current_status)
            config_content = self.create_optimized_config(optimal_config)
            
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            config_file = "intelligent_optimized_config.yaml"
            success, _, _ = self.ssh_execute(f"""cd {self.project_path} && cat > {config_file} << 'EOF'
{config_content}
EOF""")
            
            if success:
                print(f"âœ… åˆ›å»ºä¼˜åŒ–é…ç½®: {optimal_config}")
                
                # å¯åŠ¨è®­ç»ƒ
                start_cmd = f"""cd {self.project_path} && 
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256 &&
source venv/bin/activate &&
nohup python unified_complete_training_v2_fixed.py --config {config_file} > training_intelligent.log 2>&1 &"""
                
                success, output, error = self.ssh_execute(start_cmd)
                if success:
                    print("âœ… è®­ç»ƒé‡æ–°å¯åŠ¨æˆåŠŸ")
                    return True
                else:
                    print(f"âŒ å¯åŠ¨å¤±è´¥: {error}")
            
            return False
        return True
    
    def check_correlation_reports(self):
        """æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Š"""
        success, output, _ = self.ssh_execute(
            f"cd {self.project_path} && find outputs/ -name '*.json' -mtime -1 2>/dev/null | head -5"
        )
        
        if success and output.strip():
            print("ğŸ“Š å‘ç°ç›¸å…³æ€§æŠ¥å‘Š:")
            for file_path in output.strip().split('\n'):
                if file_path.strip():
                    # è¯»å–æŠ¥å‘Šå†…å®¹
                    success, content, _ = self.ssh_execute(f"cat {file_path}")
                    if success:
                        try:
                            report = json.loads(content)
                            timestamp = report.get('timestamp', 'Unknown')
                            print(f"  ğŸ“ˆ æŠ¥å‘Šæ—¶é—´: {timestamp}")
                            
                            # æ˜¾ç¤ºç›¸å…³æ€§æŒ‡æ ‡
                            for target in ['intra30m', 'nextT1d', 'ema1d']:
                                in_sample = report.get(f'{target}_in_sample_ic', 'N/A')
                                out_sample = report.get(f'{target}_out_sample_ic', 'N/A')
                                print(f"    {target}: In-sample={in_sample:.4f}, Out-sample={out_sample:.4f}")
                                
                        except json.JSONDecodeError:
                            print(f"    âš ï¸  æ— æ³•è§£ææŠ¥å‘Š: {file_path}")
    
    def monitor_continuously(self):
        """æŒç»­ç›‘æ§å’Œä¼˜åŒ–"""
        print("ğŸš€ å¯åŠ¨æ™ºèƒ½èµ„æºä¼˜åŒ–ç›‘æ§ç³»ç»Ÿ")
        print("=" * 80)
        
        last_correlation_check = 0
        last_optimization = 0
        
        while True:
            current_time = time.time()
            
            try:
                # è·å–ç³»ç»ŸçŠ¶æ€
                status = self.get_system_status()
                timestamp = status['timestamp'].strftime("%Y-%m-%d %H:%M:%S")
                
                print(f"\n[{timestamp}] ç³»ç»ŸçŠ¶æ€ç›‘æ§")
                print("-" * 60)
                
                # GPUçŠ¶æ€
                for gpu_id, gpu_info in status['gpu_status'].items():
                    mem_used = gpu_info['memory_used_mb']
                    mem_total = gpu_info['memory_total_mb']
                    util = gpu_info['utilization_pct']
                    temp = gpu_info['temperature_c']
                    mem_pct = gpu_info['memory_usage_pct']
                    
                    print(f"ğŸ”¥ GPU {gpu_id}: {mem_used}MB/{mem_total}MB ({mem_pct:.1f}%) | {util}% util | {temp}Â°C")
                
                # å†…å­˜çŠ¶æ€
                if status['memory_status']:
                    mem_info = status['memory_status']
                    print(f"ğŸ’¾ RAM: {mem_info['used_gb']}GB/{mem_info['total_gb']}GB ({mem_info['usage_pct']:.1f}%)")
                
                # è®­ç»ƒçŠ¶æ€
                if status['process_status'].get('training_active'):
                    proc_info = status['process_status']
                    print(f"âš¡ è®­ç»ƒè¿›ç¨‹: PID {proc_info['pid']} | CPU {proc_info['cpu_pct']:.1f}% | å†…å­˜ {proc_info['memory_pct']:.1f}%")
                    
                    # è®­ç»ƒæŒ‡æ ‡
                    if status['training_metrics']:
                        metrics = status['training_metrics']
                        if 'current_epoch' in metrics:
                            epoch = metrics['current_epoch']
                            iterations = metrics['iterations']
                            time_elapsed = metrics['time_elapsed']
                            time_per_it = metrics.get('time_per_iteration', 'N/A')
                            
                            print(f"ğŸ“Š è®­ç»ƒè¿›åº¦: Epoch {epoch} | {iterations} iterations | {time_elapsed} elapsed | {time_per_it}")
                            
                            # ä¼°ç®—epochå®Œæˆæ—¶é—´
                            if 'seconds_per_iteration' in metrics:
                                sec_per_it = metrics['seconds_per_iteration']
                                # å‡è®¾æ¯ä¸ªepochå¤§çº¦1000ä¸ªiteration
                                remaining_its = max(0, 1000 - iterations)
                                remaining_time = remaining_its * sec_per_it
                                eta_minutes = remaining_time / 60
                                print(f"â±ï¸  é¢„è®¡Epochå®Œæˆæ—¶é—´: {eta_minutes:.1f} åˆ†é’Ÿ")
                        
                        if metrics.get('memory_error'):
                            print("âš ï¸  æ£€æµ‹åˆ°CUDAå†…å­˜é”™è¯¯")
                else:
                    print("âŒ è®­ç»ƒè¿›ç¨‹æœªè¿è¡Œ")
                
                # é‡å¯è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
                if current_time - last_optimization > 600:  # æ¯10åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                    self.restart_training_if_needed(status)
                    last_optimization = current_time
                
                # æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Šï¼ˆæ¯2å°æ—¶ï¼‰
                if current_time - last_correlation_check >= 7200:
                    print("\nğŸ“ˆ æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Š...")
                    self.check_correlation_reports()
                    last_correlation_check = current_time
                
                # è®°å½•æ€§èƒ½å†å²
                self.performance_history.append(status)
                if len(self.performance_history) > 100:  # ä¿ç•™æœ€è¿‘100æ¡è®°å½•
                    self.performance_history.pop(0)
                
                print("-" * 60)
                
            except Exception as e:
                print(f"âŒ ç›‘æ§é”™è¯¯: {e}")
            
            # ç­‰å¾…30ç§’
            time.sleep(30)

def main():
    """ä¸»å‡½æ•°"""
    optimizer = IntelligentResourceOptimizer()
    
    try:
        optimizer.monitor_continuously()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç›‘æ§ç»“æŸ")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()