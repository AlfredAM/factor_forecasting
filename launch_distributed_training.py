#!/usr/bin/env python3
"""
4GPUåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬
å½»åº•è§£å†³GPUåˆ©ç”¨ç‡é—®é¢˜ï¼Œæœ€å¤§åŒ–ç¡¬ä»¶æ€§èƒ½
"""

import os
import subprocess
import time
import signal
import sys
from pathlib import Path

def setup_distributed_environment():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå˜é‡"""
    env_vars = {
        'MASTER_ADDR': 'localhost',
        'MASTER_PORT': '12355',
        'WORLD_SIZE': '4',
        'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True,max_split_size_mb:512',
        'OMP_NUM_THREADS': '32',  # æœ€å¤§åŒ–CPUåˆ©ç”¨ç‡
        'CUDA_VISIBLE_DEVICES': '0,1,2,3',  # ç¡®ä¿æ‰€æœ‰GPUå¯è§
        'NCCL_DEBUG': 'INFO',  # NCCLè°ƒè¯•ä¿¡æ¯
        'PYTHONUNBUFFERED': '1'
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
    
    print("âœ… åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡å·²è®¾ç½®")
    for key, value in env_vars.items():
        print(f"   {key}={value}")

def kill_old_processes():
    """æ¸…ç†æ—§çš„è®­ç»ƒè¿›ç¨‹"""
    try:
        subprocess.run(['pkill', '-f', 'unified_complete_training'], check=False)
        time.sleep(3)
        print("âœ… æ¸…ç†äº†æ—§çš„è®­ç»ƒè¿›ç¨‹")
    except Exception as e:
        print(f"æ¸…ç†è¿›ç¨‹æ—¶å‡ºé”™: {e}")

def launch_distributed_training():
    """å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ"""
    
    print("ğŸš€ å¯åŠ¨4GPUåˆ†å¸ƒå¼è®­ç»ƒ...")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_distributed_environment()
    
    # æ¸…ç†æ—§è¿›ç¨‹
    kill_old_processes()
    
    # æ„å»ºtorchrunå‘½ä»¤
    cmd = [
        'torchrun',
        '--nproc_per_node=4',
        '--master_port=12355',
        'unified_complete_training_v2_fixed.py',
        '--config', 'distributed_4gpu_config.yaml'
    ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    # å¯åŠ¨è®­ç»ƒ
    try:
        # ä½¿ç”¨nohupåœ¨åå°è¿è¡Œ
        with open('training_4gpu_distributed.log', 'w') as log_file:
            process = subprocess.Popen(
                cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                cwd='/nas/factor_forecasting',
                env=os.environ.copy()
            )
        
        print(f"âœ… åˆ†å¸ƒå¼è®­ç»ƒå·²å¯åŠ¨ï¼ŒPID: {process.pid}")
        print("ğŸ“Š ç›‘æ§å‘½ä»¤:")
        print("   tail -f /nas/factor_forecasting/training_4gpu_distributed.log")
        print("   nvidia-smi")
        
        return process.pid
        
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return None

def create_monitoring_script():
    """åˆ›å»º4GPUç›‘æ§è„šæœ¬"""
    
    monitoring_script = '''#!/usr/bin/env python3
"""
4GPUåˆ†å¸ƒå¼è®­ç»ƒç›‘æ§è„šæœ¬
"""

import subprocess
import time
import json
import re
from datetime import datetime

def get_gpu_status():
    """è·å–æ‰€æœ‰GPUçŠ¶æ€"""
    try:
        result = subprocess.run([
            'nvidia-smi', 
            '--query-gpu=index,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True)
        
        gpu_info = []
        for line in result.stdout.strip().split('\\n'):
            if line.strip():
                parts = line.split(', ')
                if len(parts) >= 6:
                    idx, mem_used, mem_total, util, temp, power = parts
                    gpu_info.append({
                        'id': int(idx),
                        'memory_used': int(mem_used),
                        'memory_total': int(mem_total),
                        'utilization': int(util),
                        'temperature': int(temp),
                        'power': float(power)
                    })
        return gpu_info
    except Exception as e:
        print(f"è·å–GPUçŠ¶æ€å¤±è´¥: {e}")
        return []

def get_training_processes():
    """è·å–è®­ç»ƒè¿›ç¨‹ä¿¡æ¯"""
    try:
        result = subprocess.run([
            'ps', 'aux'
        ], capture_output=True, text=True)
        
        processes = []
        for line in result.stdout.split('\\n'):
            if 'unified_complete_training' in line and 'grep' not in line:
                processes.append(line.strip())
        
        return processes
    except Exception as e:
        print(f"è·å–è¿›ç¨‹ä¿¡æ¯å¤±è´¥: {e}")
        return []

def get_training_log():
    """è·å–è®­ç»ƒæ—¥å¿—"""
    try:
        with open('/nas/factor_forecasting/training_4gpu_distributed.log', 'r') as f:
            lines = f.readlines()
            return ''.join(lines[-30:])
    except Exception as e:
        return f"æ—¥å¿—è¯»å–å¤±è´¥: {e}"

def extract_training_metrics(log_text):
    """æå–è®­ç»ƒæŒ‡æ ‡"""
    metrics = {}
    
    # æå–epochä¿¡æ¯
    epoch_pattern = r'Epoch (\\d+) Training: (\\d+)it \\[([^,]+), ([^]]+)\\]'
    matches = re.findall(epoch_pattern, log_text)
    if matches:
        epoch, iterations, time_elapsed, time_per_it = matches[-1]
        metrics.update({
            'current_epoch': int(epoch),
            'iterations': int(iterations),
            'time_elapsed': time_elapsed,
            'time_per_iteration': time_per_it
        })
    
    # æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒçŠ¶æ€
    if 'DDP' in log_text or 'distributed' in log_text.lower():
        metrics['distributed_active'] = True
    
    # æ£€æŸ¥é”™è¯¯
    if 'CUDA out of memory' in log_text:
        metrics['memory_error'] = True
    if 'ERROR' in log_text:
        metrics['has_errors'] = True
    
    return metrics

def monitor_4gpu_training():
    """ç›‘æ§4GPUè®­ç»ƒ"""
    print("ğŸ” 4GPUåˆ†å¸ƒå¼è®­ç»ƒç›‘æ§ç³»ç»Ÿ")
    print("=" * 80)
    
    last_correlation_check = 0
    
    while True:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"\\n[{timestamp}] 4GPUè®­ç»ƒçŠ¶æ€æ£€æŸ¥")
        print("-" * 60)
        
        # GPUçŠ¶æ€
        gpu_info = get_gpu_status()
        if gpu_info:
            print("ğŸ“Š GPUçŠ¶æ€:")
            total_memory_used = 0
            total_memory_total = 0
            active_gpus = 0
            
            for gpu in gpu_info:
                mem_percent = (gpu['memory_used'] / gpu['memory_total']) * 100
                total_memory_used += gpu['memory_used']
                total_memory_total += gpu['memory_total']
                
                if gpu['utilization'] > 0 or gpu['memory_used'] > 1000:
                    active_gpus += 1
                
                status = "ğŸŸ¢" if gpu['utilization'] > 0 else "ğŸ”´"
                print(f"   {status} GPU {gpu['id']}: {gpu['memory_used']}MB/{gpu['memory_total']}MB "
                      f"({mem_percent:.1f}%), {gpu['utilization']}% util, {gpu['temperature']}Â°C, "
                      f"{gpu['power']:.1f}W")
            
            total_mem_percent = (total_memory_used / total_memory_total) * 100
            print(f"\\nğŸ“ˆ æ€»ä½“çŠ¶æ€: {active_gpus}/4 GPUæ´»è·ƒ, "
                  f"æ€»å†…å­˜: {total_memory_used}MB/{total_memory_total}MB ({total_mem_percent:.1f}%)")
        
        # è®­ç»ƒè¿›ç¨‹
        processes = get_training_processes()
        if processes:
            print(f"\\nâœ… å‘ç° {len(processes)} ä¸ªè®­ç»ƒè¿›ç¨‹:")
            for i, proc in enumerate(processes[:4]):  # æœ€å¤šæ˜¾ç¤º4ä¸ª
                parts = proc.split()
                if len(parts) >= 11:
                    cpu_usage = parts[2]
                    mem_usage = parts[3]
                    print(f"   è¿›ç¨‹ {i+1}: CPU {cpu_usage}%, å†…å­˜ {mem_usage}%")
        else:
            print("\\nâŒ æœªå‘ç°è®­ç»ƒè¿›ç¨‹")
        
        # è®­ç»ƒæŒ‡æ ‡
        log_text = get_training_log()
        metrics = extract_training_metrics(log_text)
        
        if metrics:
            print("\\nğŸ“Š è®­ç»ƒè¿›åº¦:")
            if 'current_epoch' in metrics:
                print(f"   å½“å‰Epoch: {metrics['current_epoch']}")
                print(f"   å®Œæˆè¿­ä»£: {metrics['iterations']}")
                print(f"   å·²ç”¨æ—¶é—´: {metrics['time_elapsed']}")
                print(f"   æ¯æ¬¡è¿­ä»£: {metrics['time_per_iteration']}")
            
            if metrics.get('distributed_active'):
                print("   âœ… åˆ†å¸ƒå¼è®­ç»ƒæ´»è·ƒ")
            
            if metrics.get('memory_error'):
                print("   âš ï¸  æ£€æµ‹åˆ°å†…å­˜é”™è¯¯")
            
            if metrics.get('has_errors'):
                print("   âš ï¸  æ£€æµ‹åˆ°è®­ç»ƒé”™è¯¯")
        
        print("=" * 80)
        
        # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        time.sleep(60)

if __name__ == "__main__":
    try:
        monitor_4gpu_training()
    except KeyboardInterrupt:
        print("\\n\\nğŸ‘‹ ç›‘æ§ç»“æŸ")
'''
    
    with open('/nas/factor_forecasting/monitor_4gpu.py', 'w') as f:
        f.write(monitoring_script)
    
    print("âœ… 4GPUç›‘æ§è„šæœ¬å·²åˆ›å»º")

if __name__ == "__main__":
    print("ğŸš€ 4GPUåˆ†å¸ƒå¼è®­ç»ƒéƒ¨ç½²ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºç›‘æ§è„šæœ¬
    create_monitoring_script()
    
    # å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
    pid = launch_distributed_training()
    
    if pid:
        print("\\nğŸ‰ 4GPUåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨æˆåŠŸ!")
        print("\\nğŸ“‹ åç»­æ“ä½œ:")
        print("1. ç›‘æ§è®­ç»ƒ: python monitor_4gpu.py")
        print("2. æŸ¥çœ‹æ—¥å¿—: tail -f training_4gpu_distributed.log")
        print("3. æ£€æŸ¥GPU: watch -n 1 nvidia-smi")
    else:
        print("\\nâŒ å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
