#!/usr/bin/env python3
"""
å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒä¿®å¤è„šæœ¬
ä»æ ¹æœ¬ä¸Šè§£å†³GPUåˆ©ç”¨ç‡ä¸è¶³é—®é¢˜ï¼Œå……åˆ†åˆ©ç”¨4å¼ A10 GPU
"""

import os
import re
from pathlib import Path

def create_distributed_config():
    """åˆ›å»ºçœŸæ­£çš„å¤šGPUåˆ†å¸ƒå¼é…ç½®"""
    config_content = """# 4GPUåˆ†å¸ƒå¼è®­ç»ƒé…ç½® - å……åˆ†åˆ©ç”¨æ‰€æœ‰ç¡¬ä»¶èµ„æº
model_type: AdvancedFactorForecastingTCNAttention
input_dim: 100
hidden_dim: 1024   # å¢å¤§æ¨¡å‹ä»¥å……åˆ†åˆ©ç”¨GPU
num_layers: 16     # å¢åŠ å±‚æ•°
num_heads: 32      # å¢åŠ æ³¨æ„åŠ›å¤´æ•°
tcn_kernel_size: 7
tcn_dilation_factor: 2
dropout_rate: 0.1
attention_dropout: 0.05
target_columns: [intra30m, nextT1d, ema1d]  # æ¢å¤3ä¸ªç›®æ ‡
sequence_length: 60        # æ¢å¤å®Œæ•´åºåˆ—é•¿åº¦
epochs: 200
batch_size: 8192          # å¤§æ‰¹æ¬¡å……åˆ†åˆ©ç”¨GPU
fixed_batch_size: 8192
learning_rate: 0.0001
weight_decay: 0.01
gradient_clip_norm: 1.0
use_mixed_precision: true
accumulation_steps: 1
use_adaptive_batch_size: false
num_workers: 0            # ä¿æŒ0é¿å…å¤šè¿›ç¨‹é—®é¢˜
pin_memory: true
use_distributed: true     # å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
world_size: 4             # 4ä¸ªGPU
backend: nccl             # NCCLåç«¯ç”¨äºGPUé€šä¿¡
auto_resume: true
log_level: INFO
ic_report_interval: 7200
enable_ic_reporting: true
checkpoint_frequency: 5
save_all_checkpoints: false
output_dir: /nas/factor_forecasting/outputs
data_dir: /nas/feature_v2_10s
train_start_date: 2018-01-02
train_end_date: 2018-10-31
val_start_date: 2018-11-01
val_end_date: 2018-12-31
test_start_date: 2019-01-01
test_end_date: 2019-12-31
enforce_next_year_prediction: true
enable_yearly_rolling: true
min_train_years: 1
rolling_window_years: 1
shuffle_buffer_size: 2048

# åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
master_addr: localhost
master_port: 12355
dist_url: env://

# GPUå†…å­˜ä¼˜åŒ–
pytorch_cuda_alloc_conf: "expandable_segments:True"
"""
    
    return config_content

def create_distributed_launcher():
    """åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬"""
    launcher_content = '''#!/bin/bash
# 4GPUåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬

set -e

echo "ğŸš€ å¯åŠ¨4GPUåˆ†å¸ƒå¼è®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
export MASTER_ADDR=localhost
export MASTER_PORT=12355
export WORLD_SIZE=4

cd /nas/factor_forecasting
source venv/bin/activate

# æ¸…ç†æ—§è¿›ç¨‹
echo "æ¸…ç†æ—§è¿›ç¨‹..."
pkill -f "python.*unified_complete_training" 2>/dev/null || true
pkill -f "torchrun" 2>/dev/null || true

# ç­‰å¾…è¿›ç¨‹å®Œå…¨æ¸…ç†
sleep 5

# æ£€æŸ¥GPUçŠ¶æ€
echo "æ£€æŸ¥GPUçŠ¶æ€..."
nvidia-smi

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
echo "å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ..."
torchrun --standalone --nproc_per_node=4 \\
    unified_complete_training_v2_fixed.py \\
    --config distributed_4gpu_config.yaml \\
    > training_distributed_4gpu.log 2>&1 &

TRAIN_PID=$!
echo "åˆ†å¸ƒå¼è®­ç»ƒå·²å¯åŠ¨ï¼Œä¸»è¿›ç¨‹PID: $TRAIN_PID"

# ç­‰å¾…å‡ ç§’é’Ÿè®©è®­ç»ƒå¯åŠ¨
sleep 10

# æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
echo "æ£€æŸ¥è®­ç»ƒè¿›ç¨‹..."
ps aux | grep python | grep unified || echo "æœªæ‰¾åˆ°è®­ç»ƒè¿›ç¨‹"

echo "æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ..."
nvidia-smi

echo "âœ… åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å®Œæˆ"
'''
    
    return launcher_content

def patch_training_script_for_distributed():
    """ä¿®æ”¹è®­ç»ƒè„šæœ¬æ”¯æŒtorchrunåˆ†å¸ƒå¼å¯åŠ¨"""
    script_path = "/nas/factor_forecasting/unified_complete_training_v2_fixed.py"
    
    with open(script_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ·»åŠ torchrunæ”¯æŒ
    torchrun_patch = '''
# Torchrunåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
import os

def setup_torchrun_distributed():
    """è®¾ç½®torchrunåˆ†å¸ƒå¼ç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        # è¿è¡Œåœ¨torchrunç¯å¢ƒä¸‹
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', rank))
        
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(local_rank)
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        if not dist.is_initialized():
            dist.init_process_group(
                backend='nccl',
                init_method='env://',
                world_size=world_size,
                rank=rank
            )
        
        return rank, world_size, local_rank
    
    return 0, 1, 0
'''
    
    # åœ¨mainå‡½æ•°å‰æ·»åŠ torchrunæ”¯æŒ
    if 'def setup_torchrun_distributed' not in content:
        main_func_pos = content.find('def main():')
        if main_func_pos != -1:
            content = content[:main_func_pos] + torchrun_patch + '\n' + content[main_func_pos:]
    
    # ä¿®æ”¹mainå‡½æ•°ä»¥æ”¯æŒtorchrun
    main_pattern = r'def main\(\):(.*?)if __name__ == "__main__":'
    
    def replace_main(match):
        main_body = match.group(1)
        new_main = '''def main():
    """Main entry point with torchrun support"""
    parser = argparse.ArgumentParser(description='Unified Complete Training System')
    parser.add_argument('--config', type=str, default='server_optimized_config.yaml',
                       help='Configuration file path')
    parser.add_argument('--resume', type=str, default=None,
                       help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    
    # Setup torchrun distributed environment
    rank, world_size, local_rank = setup_torchrun_distributed()
    
    print(f"Rank {rank}/{world_size}, Local rank: {local_rank}")
    
    try:
        trainer = UnifiedCompleteTrainer(config, rank, world_size)
        if world_size > 1:
            trainer.setup_distributed()
        trainer.setup_data_loaders()
        trainer.create_model()
        
        if args.resume:
            trainer.load_checkpoint(args.resume)
        
        trainer.train()
        
    except Exception as e:
        print(f"Training failed on rank {rank}: {e}")
        raise
    finally:
        if world_size > 1 and dist.is_initialized():
            dist.destroy_process_group()

'''
        return new_main + 'if __name__ == "__main__":'
    
    content = re.sub(main_pattern, replace_main, content, flags=re.DOTALL)
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("âœ… è®­ç»ƒè„šæœ¬å·²ä¿®æ”¹ä¸ºæ”¯æŒtorchrunåˆ†å¸ƒå¼è®­ç»ƒ")

def create_monitoring_script():
    """åˆ›å»º4GPUç›‘æ§è„šæœ¬"""
    monitoring_script = '''#!/usr/bin/env python3
"""
4GPUåˆ†å¸ƒå¼è®­ç»ƒç›‘æ§è„šæœ¬
æŒç»­ç›‘æ§æ‰€æœ‰GPUä½¿ç”¨æƒ…å†µå’Œè®­ç»ƒè¿›åº¦
"""

import subprocess
import time
import json
import re
from datetime import datetime

def get_all_gpu_status():
    """è·å–æ‰€æœ‰GPUè¯¦ç»†çŠ¶æ€"""
    try:
        result = subprocess.run([
            'sshpass', '-p', 'Abab1234', 'ssh', '-o', 'StrictHostKeyChecking=no',
            'ecs-user@8.216.35.79',
            'nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu,utilization.memory,power.draw,temperature.gpu --format=csv,noheader,nounits'
        ], capture_output=True, text=True)
        
        return result.stdout.strip()
    except Exception as e:
        return f"GPUçŠ¶æ€è·å–å¤±è´¥: {e}"

def get_training_processes():
    """è·å–è®­ç»ƒè¿›ç¨‹ä¿¡æ¯"""
    try:
        result = subprocess.run([
            'sshpass', '-p', 'Abab1234', 'ssh', '-o', 'StrictHostKeyChecking=no',
            'ecs-user@8.216.35.79',
            'ps aux | grep -E "(python.*unified|torchrun)" | grep -v grep'
        ], capture_output=True, text=True)
        
        return result.stdout.strip()
    except Exception as e:
        return f"è¿›ç¨‹ä¿¡æ¯è·å–å¤±è´¥: {e}"

def get_training_log():
    """è·å–è®­ç»ƒæ—¥å¿—"""
    try:
        result = subprocess.run([
            'sshpass', '-p', 'Abab1234', 'ssh', '-o', 'StrictHostKeyChecking=no',
            'ecs-user@8.216.35.79',
            'cd /nas/factor_forecasting && tail -30 training_distributed_4gpu.log 2>/dev/null || echo "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"'
        ], capture_output=True, text=True)
        
        return result.stdout.strip()
    except Exception as e:
        return f"æ—¥å¿—è·å–å¤±è´¥: {e}"

def extract_epoch_time(log_text):
    """æå–epochå®Œæˆæ—¶é—´"""
    # æŸ¥æ‰¾epochå®Œæˆä¿¡æ¯
    epoch_pattern = r'Epoch (\d+) completed.*?time: ([\\d\\.]+)s'
    matches = re.findall(epoch_pattern, log_text)
    
    if matches:
        return matches[-1]  # è¿”å›æœ€æ–°çš„epochä¿¡æ¯
    
    # æŸ¥æ‰¾è®­ç»ƒè¿›åº¦ä¿¡æ¯
    progress_pattern = r'Epoch (\d+) Training: (\d+)it \\[([^,]+), ([^]]+)\\]'
    matches = re.findall(progress_pattern, log_text)
    
    if matches:
        return matches[-1]
    
    return None

def check_correlation_reports():
    """æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Š"""
    try:
        result = subprocess.run([
            'sshpass', '-p', 'Abab1234', 'ssh', '-o', 'StrictHostKeyChecking=no',
            'ecs-user@8.216.35.79',
            'cd /nas/factor_forecasting && find outputs/ -name "*correlation*" -type f -mtime -1 2>/dev/null | head -5'
        ], capture_output=True, text=True)
        
        return result.stdout.strip()
    except Exception as e:
        return f"ç›¸å…³æ€§æŠ¥å‘Šæ£€æŸ¥å¤±è´¥: {e}"

def monitor_4gpu_training():
    """ç›‘æ§4GPUåˆ†å¸ƒå¼è®­ç»ƒ"""
    print("ğŸ” 4GPUåˆ†å¸ƒå¼è®­ç»ƒç›‘æ§ç³»ç»Ÿå¯åŠ¨")
    print("=" * 100)
    
    last_correlation_check = 0
    
    while True:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        current_time = time.time()
        
        print(f"\\n[{timestamp}] 4GPUåˆ†å¸ƒå¼è®­ç»ƒçŠ¶æ€æ£€æŸ¥")
        print("=" * 100)
        
        # æ£€æŸ¥è®­ç»ƒè¿›ç¨‹
        processes = get_training_processes()
        if processes:
            print("âœ… è®­ç»ƒè¿›ç¨‹è¿è¡Œä¸­:")
            for line in processes.split('\\n'):
                if line.strip():
                    print(f"  {line}")
        else:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒè¿›ç¨‹")
        
        # æ£€æŸ¥æ‰€æœ‰GPUçŠ¶æ€
        gpu_status = get_all_gpu_status()
        print("\\nğŸ“Š æ‰€æœ‰GPUä½¿ç”¨çŠ¶æ€:")
        print("GPU | æ˜¾å­˜ä½¿ç”¨    | GPUåˆ©ç”¨ç‡ | å†…å­˜åˆ©ç”¨ç‡ | åŠŸè€—  | æ¸©åº¦")
        print("-" * 70)
        
        total_memory_used = 0
        total_memory_total = 0
        active_gpus = 0
        
        for line in gpu_status.split('\\n'):
            if line.strip() and ',' in line:
                parts = [p.strip() for p in line.split(',')]
                if len(parts) >= 8:
                    gpu_id, name, mem_used, mem_total, util_gpu, util_mem, power, temp = parts
                    total_memory_used += int(mem_used)
                    total_memory_total += int(mem_total)
                    
                    if int(util_gpu) > 0:
                        active_gpus += 1
                        status = "ğŸŸ¢"
                    else:
                        status = "ğŸ”´"
                    
                    print(f"{status} {gpu_id}  | {mem_used:>4}MB/{mem_total:>5}MB | {util_gpu:>6}%    | {util_mem:>7}%     | {power:>4}W | {temp:>2}Â°C")
        
        # æ€»ç»“GPUä½¿ç”¨æƒ…å†µ
        total_util = (total_memory_used / total_memory_total) * 100 if total_memory_total > 0 else 0
        print(f"\\nğŸ“ˆ GPUä½¿ç”¨æ€»ç»“: {active_gpus}/4ä¸ªGPUæ´»è·ƒ, æ€»æ˜¾å­˜åˆ©ç”¨ç‡: {total_util:.1f}%")
        
        # æ£€æŸ¥è®­ç»ƒæ—¥å¿—
        log_text = get_training_log()
        epoch_info = extract_epoch_time(log_text)
        
        if epoch_info:
            print(f"\\nâ±ï¸  è®­ç»ƒè¿›åº¦: {epoch_info}")
        
        # æ£€æŸ¥é”™è¯¯
        if 'error' in log_text.lower() or 'failed' in log_text.lower():
            print("âš ï¸  æ£€æµ‹åˆ°è®­ç»ƒé”™è¯¯ï¼ŒæŸ¥çœ‹æ—¥å¿—è·å–è¯¦æƒ…")
        
        # æ¯2å°æ—¶æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Š
        if current_time - last_correlation_check >= 7200:
            print("\\nğŸ“Š æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Š...")
            correlation_files = check_correlation_reports()
            if correlation_files:
                print(f"å‘ç°ç›¸å…³æ€§æŠ¥å‘Š: {correlation_files}")
            else:
                print("æš‚æ— æ–°çš„ç›¸å…³æ€§æŠ¥å‘Š")
            last_correlation_check = current_time
        
        print("=" * 100)
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    try:
        monitor_4gpu_training()
    except KeyboardInterrupt:
        print("\\n\\nğŸ‘‹ ç›‘æ§ç»“æŸ")
    except Exception as e:
        print(f"\\nâŒ ç›‘æ§é”™è¯¯: {e}")
'''
    
    return monitoring_script

def apply_distributed_fixes():
    """åº”ç”¨æ‰€æœ‰åˆ†å¸ƒå¼ä¿®å¤"""
    print("ğŸš€ å¼€å§‹é…ç½®4GPUåˆ†å¸ƒå¼è®­ç»ƒ...")
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config_content = create_distributed_config()
    with open("/nas/factor_forecasting/distributed_4gpu_config.yaml", "w") as f:
        f.write(config_content)
    print("âœ… åˆ›å»ºäº†4GPUåˆ†å¸ƒå¼é…ç½®")
    
    # åˆ›å»ºå¯åŠ¨è„šæœ¬
    launcher_content = create_distributed_launcher()
    with open("/nas/factor_forecasting/launch_4gpu_distributed.sh", "w") as f:
        f.write(launcher_content)
    print("âœ… åˆ›å»ºäº†åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬")
    
    # ä¿®æ”¹è®­ç»ƒè„šæœ¬
    patch_training_script_for_distributed()
    
    # åˆ›å»ºç›‘æ§è„šæœ¬
    monitoring_content = create_monitoring_script()
    with open("/nas/factor_forecasting/monitor_4gpu.py", "w") as f:
        f.write(monitoring_content)
    print("âœ… åˆ›å»ºäº†4GPUç›‘æ§è„šæœ¬")
    
    print("ğŸ‰ 4GPUåˆ†å¸ƒå¼è®­ç»ƒé…ç½®å®Œæˆ!")
    print("ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨:")
    print("bash /nas/factor_forecasting/launch_4gpu_distributed.sh")

if __name__ == "__main__":
    apply_distributed_fixes()
"""
