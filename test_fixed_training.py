#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„è®­ç»ƒè„šæœ¬
éªŒè¯æ‰€æœ‰å…³é”®é—®é¢˜æ˜¯å¦å·²è§£å†³
"""

import os
import sys
import torch
import torch.multiprocessing as mp
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å¯¼å…¥æ˜¯å¦æ­£å¸¸"""
    print("=== æµ‹è¯•å¯¼å…¥ ===")
    try:
        from src.unified_complete_training_v2_fixed import UnifiedCompleteTrainer, load_config, main
        print("âœ… è®­ç»ƒè„šæœ¬å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\n=== æµ‹è¯•é…ç½®åŠ è½½ ===")
    try:
        from src.unified_complete_training_v2_fixed import load_config
        
        # æµ‹è¯•é»˜è®¤é…ç½®
        config = load_config("nonexistent.yaml")
        print(f"âœ… é»˜è®¤é…ç½®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(config)} ä¸ªå‚æ•°")
        
        # æ£€æŸ¥å…³é”®å‚æ•°
        required_keys = ['batch_size', 'num_workers', 'use_distributed']
        for key in required_keys:
            if key in config:
                print(f"âœ… å…³é”®å‚æ•° {key}: {config[key]}")
            else:
                print(f"âŒ ç¼ºå°‘å…³é”®å‚æ•°: {key}")
                return False
        return True
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def test_multiprocessing():
    """æµ‹è¯•å¤šè¿›ç¨‹åŠŸèƒ½"""
    print("\n=== æµ‹è¯•å¤šè¿›ç¨‹åŠŸèƒ½ ===")
    try:
        # è®¾ç½®spawnæ–¹æ³•
        mp.set_start_method('spawn', force=True)
        print("âœ… multiprocessing spawnæ–¹æ³•è®¾ç½®æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•çš„å¤šè¿›ç¨‹ä»»åŠ¡
        def simple_worker(rank):
            return f"Worker {rank} completed"
        
        # ä¸å®é™…å¯åŠ¨è¿›ç¨‹ï¼Œåªæµ‹è¯•è®¾ç½®
        print("âœ… å¤šè¿›ç¨‹è®¾ç½®éªŒè¯æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å¤šè¿›ç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_cuda_availability():
    """æµ‹è¯•CUDAå¯ç”¨æ€§"""
    print("\n=== æµ‹è¯•CUDAç¯å¢ƒ ===")
    try:
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print("âœ… CUDAç¯å¢ƒæ£€æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ CUDAç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_trainer_initialization():
    """æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–"""
    print("\n=== æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ– ===")
    try:
        from src.unified_complete_training_v2_fixed import UnifiedCompleteTrainer, load_config
        
        # åŠ è½½é…ç½®
        config = load_config("nonexistent.yaml")
        config.update({
            'batch_size': 32,
            'num_workers': 2,
            'epochs': 1,
            'data_dir': './test_data'  # ä½¿ç”¨æœ¬åœ°æµ‹è¯•ç›®å½•
        })
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = UnifiedCompleteTrainer(config, rank=0, world_size=1)
        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"è®¾å¤‡: {trainer.device}")
        print(f"é…ç½®å‚æ•°æ•°é‡: {len(trainer.config)}")
        
        return True
    except Exception as e:
        print(f"âŒ è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹æµ‹è¯•ä¿®å¤åçš„è®­ç»ƒè„šæœ¬...")
    
    tests = [
        test_imports,
        test_config_loading,
        test_multiprocessing,
        test_cuda_availability,
        test_trainer_initialization
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"æµ‹è¯•å¤±è´¥: {test_func.__name__}")
        except Exception as e:
            print(f"æµ‹è¯•å¼‚å¸¸ {test_func.__name__}: {e}")
    
    print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"é€šè¿‡: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤åçš„è„šæœ¬å¯ä»¥ä½¿ç”¨ã€‚")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
