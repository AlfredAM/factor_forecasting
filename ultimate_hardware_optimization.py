#!/usr/bin/env python3
"""
ç»ˆæç¡¬ä»¶ä¼˜åŒ–è„šæœ¬
å½»åº•è§£å†³GPUåˆ©ç”¨ç‡ã€å†…å­˜ç¢ç‰‡åŒ–å’Œåˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜
"""

import os
import sys
import subprocess
import time
from pathlib import Path

class HardwareOptimizer:
    def __init__(self):
        self.gpu_count = 4
        self.total_memory_per_gpu = 23028  # MB
        self.safe_memory_per_gpu = 20000   # MBï¼Œç•™å‡ºå®‰å…¨è¾¹ç•Œ
        
    def analyze_current_problems(self):
        """åˆ†æå½“å‰é—®é¢˜"""
        print("ğŸ” æ·±åº¦åˆ†æå½“å‰ç¡¬ä»¶åˆ©ç”¨ç‡é—®é¢˜...")
        
        problems = []
        
        # æ£€æŸ¥GPUå†…å­˜ç¢ç‰‡åŒ–
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                for i, line in enumerate(lines):
                    used, total = map(int, line.split(', '))
                    usage_percent = (used / total) * 100
                    if usage_percent > 90:
                        problems.append(f"GPU {i}: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_percent:.1f}%)")
                    elif usage_percent > 30 and usage_percent < 50:
                        problems.append(f"GPU {i}: å¯èƒ½å­˜åœ¨å†…å­˜ç¢ç‰‡åŒ–")
        except Exception as e:
            problems.append(f"GPUæ£€æŸ¥å¤±è´¥: {e}")
        
        # æ£€æŸ¥è¿›ç¨‹åˆ†å¸ƒ
        try:
            result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
            python_processes = [line for line in result.stdout.split('\n') 
                              if 'unified_complete_training' in line and 'grep' not in line]
            if len(python_processes) > 1:
                problems.append(f"æ£€æµ‹åˆ°{len(python_processes)}ä¸ªè®­ç»ƒè¿›ç¨‹ï¼Œå¯èƒ½å­˜åœ¨èµ„æºç«äº‰")
        except Exception as e:
            problems.append(f"è¿›ç¨‹æ£€æŸ¥å¤±è´¥: {e}")
        
        return problems
    
    def create_optimized_distributed_config(self):
        """åˆ›å»ºä¼˜åŒ–çš„åˆ†å¸ƒå¼é…ç½®"""
        config_content = f"""# ä¼˜åŒ–çš„4GPUåˆ†å¸ƒå¼é…ç½®
model_type: AdvancedFactorForecastingTCNAttention
input_dim: 100
hidden_dim: 512    # å‡å°æ¨¡å‹å¤§å°ä»¥é€‚åº”4GPUåˆ†å¸ƒå¼
num_layers: 8      # é€‚ä¸­å±‚æ•°
num_heads: 8       # é€‚ä¸­æ³¨æ„åŠ›å¤´æ•°
tcn_kernel_size: 3
tcn_dilation_factor: 2
dropout_rate: 0.1
attention_dropout: 0.05
target_columns: [intra30m, nextT1d, ema1d]
sequence_length: 30        # å‡å°åºåˆ—é•¿åº¦
epochs: 200
batch_size: 512           # æ¯GPUæ‰¹æ¬¡å¤§å°
fixed_batch_size: 512
learning_rate: 0.0001
weight_decay: 0.01
gradient_clip_norm: 1.0
use_mixed_precision: true
accumulation_steps: 1     # åˆ†å¸ƒå¼è®­ç»ƒä¸éœ€è¦æ¢¯åº¦ç´¯ç§¯
use_adaptive_batch_size: false
num_workers: 0
pin_memory: false
use_distributed: true     # å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
auto_resume: true
log_level: INFO
ic_report_interval: 7200
enable_ic_reporting: true
checkpoint_frequency: 10
save_all_checkpoints: false
output_dir: /nas/factor_forecasting/outputs
data_dir: /nas/feature_v2_10s
train_start_date: 2018-01-02
train_end_date: 2018-10-31   # ä½¿ç”¨å‰10ä¸ªæœˆæ•°æ®
val_start_date: 2018-11-01
val_end_date: 2018-11-30
test_start_date: 2018-12-01
test_end_date: 2018-12-31
enforce_next_year_prediction: false
enable_yearly_rolling: false
min_train_years: 1
rolling_window_years: 1
shuffle_buffer_size: 256

# åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
world_size: 4
backend: nccl

# å†…å­˜ä¼˜åŒ–å‚æ•°
max_memory_usage_per_gpu: 18    # æ¯GPUæœ€å¤§å†…å­˜ä½¿ç”¨
streaming_chunk_size: 10000     # å°chunké¿å…å†…å­˜å³°å€¼
enable_memory_mapping: false    # ç¦ç”¨å†…å­˜æ˜ å°„å‡å°‘ç¢ç‰‡
enable_gradient_checkpointing: true

# PyTorchä¼˜åŒ–
torch_compile: false
enable_flash_attention: false
use_channels_last: false  # åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯èƒ½æœ‰é—®é¢˜
"""
        
        with open('/tmp/optimized_4gpu_config.yaml', 'w') as f:
            f.write(config_content)
        
        return '/tmp/optimized_4gpu_config.yaml'
    
    def create_memory_optimized_training_script(self):
        """åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬è¡¥ä¸"""
        patch_content = '''
# å†…å­˜ä¼˜åŒ–è¡¥ä¸
import torch
import gc
import os

# è®¾ç½®PyTorchå†…å­˜ç®¡ç†
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:True'

def optimize_memory_settings():
    """ä¼˜åŒ–å†…å­˜è®¾ç½®"""
    if torch.cuda.is_available():
        # å¯ç”¨å†…å­˜æ± 
        torch.cuda.set_per_process_memory_fraction(0.85)  # æ¯ä¸ªè¿›ç¨‹æœ€å¤šä½¿ç”¨85%æ˜¾å­˜
        
        # æ¸…ç©ºç¼“å­˜
        torch.cuda.empty_cache()
        
        # è®¾ç½®å†…å­˜å¢é•¿æ¨¡å¼
        torch.cuda.memory.set_per_process_memory_fraction(0.85)

def aggressive_cleanup():
    """æ¿€è¿›çš„å†…å­˜æ¸…ç†"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        # é‡ç½®å³°å€¼å†…å­˜ç»Ÿè®¡
        torch.cuda.reset_peak_memory_stats()

# åœ¨è®­ç»ƒå¼€å§‹å‰è°ƒç”¨
optimize_memory_settings()
'''
        
        return patch_content
    
    def create_distributed_launcher(self):
        """åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨"""
        launcher_content = f'''#!/bin/bash
# 4GPUåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨

set -e

export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:256,expandable_segments:True"
export NCCL_DEBUG=INFO
export NCCL_TREE_THRESHOLD=0
export OMP_NUM_THREADS=8  # é™åˆ¶CPUçº¿ç¨‹é¿å…è¿‡åº¦ç«äº‰

cd /nas/factor_forecasting
source venv/bin/activate

# æ¸…ç†æ—§è¿›ç¨‹
pkill -f unified_complete_training 2>/dev/null || true
pkill -f torchrun 2>/dev/null || true
sleep 5

# æ¸…ç†GPUå†…å­˜
python -c "import torch; [torch.cuda.empty_cache() for _ in range(4) if torch.cuda.is_available()]" 2>/dev/null || true

echo "å¯åŠ¨4GPUåˆ†å¸ƒå¼è®­ç»ƒ..."
echo "é…ç½®æ–‡ä»¶: optimized_4gpu_config.yaml"
echo "æ•°æ®èŒƒå›´: 2018å¹´å‰10ä¸ªæœˆ"

# ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun \\
    --nproc_per_node=4 \\
    --master_port=12355 \\
    unified_complete_training_v2_fixed.py \\
    --config optimized_4gpu_config.yaml \\
    > training_4gpu_optimized.log 2>&1 &

TRAIN_PID=$!
echo "è®­ç»ƒå·²å¯åŠ¨ï¼Œä¸»è¿›ç¨‹PID: $TRAIN_PID"
echo "æ—¥å¿—æ–‡ä»¶: training_4gpu_optimized.log"
echo "ç›‘æ§å‘½ä»¤: tail -f training_4gpu_optimized.log"

# ç­‰å¾…è¿›ç¨‹å¯åŠ¨
sleep 10
echo "æ£€æŸ¥è¿›ç¨‹çŠ¶æ€..."
ps aux | grep unified_complete_training | grep -v grep || echo "è­¦å‘Š: è®­ç»ƒè¿›ç¨‹å¯èƒ½æœªæ­£å¸¸å¯åŠ¨"
'''
        
        with open('/tmp/launch_4gpu_optimized.sh', 'w') as f:
            f.write(launcher_content)
        
        os.chmod('/tmp/launch_4gpu_optimized.sh', 0o755)
        return '/tmp/launch_4gpu_optimized.sh'
    
    def create_comprehensive_monitor(self):
        """åˆ›å»ºç»¼åˆç›‘æ§è„šæœ¬"""
        monitor_content = '''#!/usr/bin/env python3
"""
ç»¼åˆç¡¬ä»¶åˆ©ç”¨ç‡ç›‘æ§
å®æ—¶ç›‘æ§4GPUåˆ†å¸ƒå¼è®­ç»ƒçš„ç¡¬ä»¶åˆ©ç”¨ç‡å’Œç›¸å…³æ€§æŠ¥å‘Š
"""

import subprocess
import time
import json
import re
from datetime import datetime
from pathlib import Path

class ComprehensiveMonitor:
    def __init__(self):
        self.last_correlation_check = 0
        self.gpu_count = 4
        
    def get_detailed_gpu_status(self):
        """è·å–è¯¦ç»†GPUçŠ¶æ€"""
        try:
            # GPUåˆ©ç”¨ç‡å’Œå†…å­˜
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=index,name,memory.used,memory.total,utilization.gpu,utilization.memory,temperature.gpu,power.draw',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)
            
            gpu_stats = []
            if result.returncode == 0:
                for line in result.stdout.strip().split('\\n'):
                    if line.strip():
                        parts = line.split(', ')
                        if len(parts) >= 7:
                            gpu_stats.append({
                                'id': parts[0],
                                'name': parts[1],
                                'mem_used': int(parts[2]),
                                'mem_total': int(parts[3]),
                                'gpu_util': int(parts[4]),
                                'mem_util': int(parts[5]) if parts[5] != '[N/A]' else 0,
                                'temp': int(parts[6]),
                                'power': float(parts[7]) if parts[7] != '[N/A]' else 0
                            })
            
            return gpu_stats
        except Exception as e:
            return [{'error': str(e)}]
    
    def get_training_processes(self):
        """è·å–è®­ç»ƒè¿›ç¨‹ä¿¡æ¯"""
        try:
            result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
            processes = []
            
            for line in result.stdout.split('\\n'):
                if 'unified_complete_training' in line and 'grep' not in line:
                    parts = line.split()
                    if len(parts) >= 11:
                        processes.append({
                            'pid': parts[1],
                            'cpu_percent': parts[2],
                            'mem_percent': parts[3],
                            'command': ' '.join(parts[10:])
                        })
            
            return processes
        except Exception as e:
            return [{'error': str(e)}]
    
    def get_system_resources(self):
        """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            # CPUä¿¡æ¯
            cpu_result = subprocess.run(['top', '-bn1'], capture_output=True, text=True)
            cpu_usage = 0
            for line in cpu_result.stdout.split('\\n'):
                if 'Cpu(s):' in line:
                    # æå–CPUä½¿ç”¨ç‡
                    match = re.search(r'(\\d+\\.\\d+)%us', line)
                    if match:
                        cpu_usage = float(match.group(1))
                    break
            
            # å†…å­˜ä¿¡æ¯
            mem_result = subprocess.run(['free', '-m'], capture_output=True, text=True)
            mem_info = {}
            for line in mem_result.stdout.split('\\n'):
                if line.startswith('Mem:'):
                    parts = line.split()
                    mem_info = {
                        'total': int(parts[1]),
                        'used': int(parts[2]),
                        'free': int(parts[3]),
                        'usage_percent': (int(parts[2]) / int(parts[1])) * 100
                    }
                    break
            
            return {'cpu_usage': cpu_usage, 'memory': mem_info}
        except Exception as e:
            return {'error': str(e)}
    
    def get_training_progress(self):
        """è·å–è®­ç»ƒè¿›åº¦"""
        try:
            log_file = Path('/nas/factor_forecasting/training_4gpu_optimized.log')
            if log_file.exists():
                with open(log_file, 'r') as f:
                    lines = f.readlines()
                    recent_lines = ''.join(lines[-100:])  # æœ€å100è¡Œ
                
                # æå–epochä¿¡æ¯
                epoch_pattern = r'Epoch (\\d+) Training: (\\d+)it \\[([^,]+), ([^]]+)\\]'
                matches = re.findall(epoch_pattern, recent_lines)
                
                if matches:
                    epoch, iterations, time_elapsed, time_per_it = matches[-1]
                    return {
                        'epoch': int(epoch),
                        'iterations': int(iterations),
                        'time_elapsed': time_elapsed,
                        'time_per_iteration': time_per_it,
                        'has_errors': 'ERROR' in recent_lines,
                        'memory_errors': 'CUDA out of memory' in recent_lines
                    }
            
            return {'status': 'no_progress_found'}
        except Exception as e:
            return {'error': str(e)}
    
    def check_correlations(self):
        """æ£€æŸ¥ç›¸å…³æ€§æŠ¥å‘Š"""
        try:
            output_dir = Path('/nas/factor_forecasting/outputs')
            if output_dir.exists():
                json_files = list(output_dir.glob('**/*.json'))
                if json_files:
                    latest_file = max(json_files, key=lambda p: p.stat().st_mtime)
                    with open(latest_file, 'r') as f:
                        data = json.load(f)
                    
                    correlations = {}
                    timestamp = data.get('timestamp', 'unknown')
                    
                    if 'correlations' in data:
                        for target, corr_data in data['correlations'].items():
                            if isinstance(corr_data, dict):
                                for metric, value in corr_data.items():
                                    if 'ic' in metric.lower():
                                        correlations[f'{target}_{metric}'] = value
                    
                    return {'correlations': correlations, 'timestamp': timestamp}
        except Exception as e:
            return {'error': str(e)}
        
        return {'status': 'no_correlations_found'}
    
    def monitor_continuously(self):
        """æŒç»­ç›‘æ§"""
        print("ğŸš€ å¯åŠ¨4GPUåˆ†å¸ƒå¼è®­ç»ƒç»¼åˆç›‘æ§ç³»ç»Ÿ")
        print("=" * 100)
        
        while True:
            current_time = time.time()
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            print(f"\\n[{timestamp}] ç¡¬ä»¶åˆ©ç”¨ç‡ç›‘æ§æŠ¥å‘Š")
            print("=" * 100)
            
            # GPUçŠ¶æ€
            gpu_stats = self.get_detailed_gpu_status()
            print("\\nğŸ“Š GPUåˆ©ç”¨ç‡è¯¦æƒ…:")
            total_gpu_util = 0
            total_mem_util = 0
            
            for gpu in gpu_stats:
                if 'error' not in gpu:
                    mem_percent = (gpu['mem_used'] / gpu['mem_total']) * 100
                    print(f"   GPU {gpu['id']}: è®¡ç®—åˆ©ç”¨ç‡ {gpu['gpu_util']}%, "
                          f"æ˜¾å­˜ {gpu['mem_used']}MB/{gpu['mem_total']}MB ({mem_percent:.1f}%), "
                          f"æ¸©åº¦ {gpu['temp']}Â°C, åŠŸè€— {gpu['power']:.1f}W")
                    total_gpu_util += gpu['gpu_util']
                    total_mem_util += mem_percent
            
            if len(gpu_stats) > 0 and 'error' not in gpu_stats[0]:
                avg_gpu_util = total_gpu_util / len(gpu_stats)
                avg_mem_util = total_mem_util / len(gpu_stats)
                print(f"   å¹³å‡GPUåˆ©ç”¨ç‡: {avg_gpu_util:.1f}%, å¹³å‡æ˜¾å­˜åˆ©ç”¨ç‡: {avg_mem_util:.1f}%")
            
            # ç³»ç»Ÿèµ„æº
            system_resources = self.get_system_resources()
            if 'error' not in system_resources:
                print(f"\\nğŸ’» ç³»ç»Ÿèµ„æº:")
                print(f"   CPUåˆ©ç”¨ç‡: {system_resources['cpu_usage']:.1f}%")
                if 'memory' in system_resources:
                    mem = system_resources['memory']
                    print(f"   å†…å­˜ä½¿ç”¨: {mem['used']}MB/{mem['total']}MB ({mem['usage_percent']:.1f}%)")
            
            # è®­ç»ƒè¿›ç¨‹
            processes = self.get_training_processes()
            print(f"\\nğŸƒ è®­ç»ƒè¿›ç¨‹çŠ¶æ€:")
            if processes:
                for proc in processes:
                    if 'error' not in proc:
                        print(f"   PID {proc['pid']}: CPU {proc['cpu_percent']}%, å†…å­˜ {proc['mem_percent']}%")
            else:
                print("   âŒ æœªæ£€æµ‹åˆ°è®­ç»ƒè¿›ç¨‹")
            
            # è®­ç»ƒè¿›åº¦
            progress = self.get_training_progress()
            if 'error' not in progress and 'epoch' in progress:
                print(f"\\nğŸ“ˆ è®­ç»ƒè¿›åº¦:")
                print(f"   å½“å‰Epoch: {progress['epoch']}")
                print(f"   å®Œæˆè¿­ä»£: {progress['iterations']}")
                print(f"   å·²ç”¨æ—¶é—´: {progress['time_elapsed']}")
                print(f"   æ¯æ¬¡è¿­ä»£: {progress['time_per_iteration']}")
                
                if progress.get('memory_errors'):
                    print("   âš ï¸  æ£€æµ‹åˆ°å†…å­˜é”™è¯¯")
                elif progress.get('has_errors'):
                    print("   âš ï¸  æ£€æµ‹åˆ°è®­ç»ƒé”™è¯¯")
                else:
                    print("   âœ… è®­ç»ƒæ­£å¸¸è¿›è¡Œ")
            
            # æ¯2å°æ—¶æ£€æŸ¥ç›¸å…³æ€§
            if current_time - self.last_correlation_check >= 7200:
                print(f"\\nğŸ“Š ç›¸å…³æ€§æŠ¥å‘Šæ£€æŸ¥...")
                correlation_data = self.check_correlations()
                
                if 'correlations' in correlation_data:
                    print(f"   æŠ¥å‘Šæ—¶é—´: {correlation_data['timestamp']}")
                    print("   ç›¸å…³æ€§æ•°æ®:")
                    for metric, value in correlation_data['correlations'].items():
                        print(f"     {metric}: {value:.4f}")
                else:
                    print("   æš‚æ— ç›¸å…³æ€§æ•°æ®")
                
                self.last_correlation_check = current_time
            
            print("=" * 100)
            
            # æ¯1åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            time.sleep(60)

if __name__ == "__main__":
    monitor = ComprehensiveMonitor()
    try:
        monitor.monitor_continuously()
    except KeyboardInterrupt:
        print("\\n\\nğŸ‘‹ ç›‘æ§ç»“æŸ")
'''
        
        with open('/tmp/comprehensive_monitor.py', 'w') as f:
            f.write(monitor_content)
        
        return '/tmp/comprehensive_monitor.py'
    
    def deploy_optimization(self):
        """éƒ¨ç½²ä¼˜åŒ–æ–¹æ¡ˆ"""
        print("ğŸš€ å¼€å§‹éƒ¨ç½²ç»ˆæç¡¬ä»¶ä¼˜åŒ–æ–¹æ¡ˆ...")
        
        # åˆ†æå½“å‰é—®é¢˜
        problems = self.analyze_current_problems()
        if problems:
            print("å‘ç°çš„é—®é¢˜:")
            for problem in problems:
                print(f"  âŒ {problem}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_path = self.create_optimized_distributed_config()
        print(f"âœ… åˆ›å»ºä¼˜åŒ–é…ç½®: {config_path}")
        
        # åˆ›å»ºå¯åŠ¨è„šæœ¬
        launcher_path = self.create_distributed_launcher()
        print(f"âœ… åˆ›å»ºåˆ†å¸ƒå¼å¯åŠ¨å™¨: {launcher_path}")
        
        # åˆ›å»ºç›‘æ§è„šæœ¬
        monitor_path = self.create_comprehensive_monitor()
        print(f"âœ… åˆ›å»ºç»¼åˆç›‘æ§: {monitor_path}")
        
        print("\\nğŸ¯ ä¼˜åŒ–æ–¹æ¡ˆéƒ¨ç½²å®Œæˆ!")
        print("\\nä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. ä¸Šä¼ é…ç½®æ–‡ä»¶åˆ°æœåŠ¡å™¨")
        print("2. åœæ­¢å½“å‰è®­ç»ƒè¿›ç¨‹")
        print("3. å¯åŠ¨ä¼˜åŒ–çš„4GPUåˆ†å¸ƒå¼è®­ç»ƒ")
        print("4. å¯åŠ¨ç»¼åˆç›‘æ§")
        
        return {
            'config': config_path,
            'launcher': launcher_path,
            'monitor': monitor_path
        }

if __name__ == "__main__":
    optimizer = HardwareOptimizer()
    result = optimizer.deploy_optimization()
    print(f"\\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for key, path in result.items():
        print(f"  {key}: {path}")
