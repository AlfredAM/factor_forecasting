#!/usr/bin/env python3
"""
ç»ˆæå†…å­˜å®‰å…¨è®­ç»ƒè„šæœ¬ - å®Œå…¨ç‹¬ç«‹ï¼Œå½»åº•è§£å†³æ‰€æœ‰é—®é¢˜
"""

import os
import sys
import gc
import time
import yaml
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch.multiprocessing as mp
import numpy as np
from datetime import datetime

# è®¾ç½®CUDAå†…å­˜ç®¡ç†
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass

# å¯ç”¨CUDAå†…å­˜ä¼˜åŒ–
torch.cuda.empty_cache()
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.7)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

sys.path.insert(0, '/nas/factor_forecasting/src')

def cleanup_memory():
    """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

class SimpleCorrelationLoss(nn.Module):
    """ç®€å•çš„ç›¸å…³æ€§æŸå¤±å‡½æ•°"""
    
    def __init__(self):
        super().__init__()
        self.mse = nn.MSELoss()
        
    def forward(self, predictions, targets):
        if isinstance(predictions, dict):
            predictions = predictions['nextT1d']
        if isinstance(targets, dict):
            targets = targets['nextT1d']
            
        # ç®€å•çš„MSEæŸå¤±
        return self.mse(predictions, targets)

class MemorySafeTrainer:
    """å†…å­˜å®‰å…¨è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.optimizer = None
        self.criterion = SimpleCorrelationLoss()
        self.scaler = torch.cuda.amp.GradScaler() if config.get('use_mixed_precision', True) else None
        
        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
    
    def create_model(self):
        """åˆ›å»ºç®€åŒ–æ¨¡å‹"""
        from src.models.advanced_tcn_attention import create_advanced_model
        
        # åˆ›å»ºæ¨¡å‹é…ç½®
        model_config = {
            'input_dim': self.config['input_dim'],
            'hidden_dim': self.config['hidden_dim'],
            'num_layers': self.config['num_layers'],
            'num_heads': self.config['num_heads'],
            'tcn_kernel_size': self.config['tcn_kernel_size'],
            'tcn_dilation_factor': self.config['tcn_dilation_factor'],
            'dropout_rate': self.config['dropout_rate'],
            'attention_dropout': self.config['attention_dropout'],
            'target_columns': self.config['target_columns'],
            'sequence_length': self.config['sequence_length']
        }
        
        self.model = create_advanced_model(model_config)
        self.model.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"æ¨¡å‹åˆ›å»ºå®Œæˆ - å‚æ•°æ•°é‡: {total_params:,}")
        
        cleanup_memory()
    
    def create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        from src.data_processing.optimized_streaming_loader import create_optimized_dataloaders
        from src.data_processing.adaptive_memory_manager import create_memory_manager
        
        # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
        memory_manager = create_memory_manager({
            'max_memory_usage_gb': 200,  # å‡å°‘å†…å­˜ä½¿ç”¨
            'enable_monitoring': True
        })
        
        # è·å–å› å­åˆ—åï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        factor_columns = [f'factor_{i}' for i in range(self.config['input_dim'])]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader, self.test_loader = create_optimized_dataloaders(
            data_dir=self.config['data_dir'],
            factor_columns=factor_columns,
            target_columns=self.config['target_columns'],
            train_dates=(self.config['train_start_date'], self.config['train_end_date']),
            val_dates=(self.config['val_start_date'], self.config['val_end_date']),
            test_dates=(self.config['test_start_date'], self.config['test_end_date']),
            sequence_length=self.config['sequence_length'],
            batch_size=self.config['batch_size'],
            num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹
            memory_config={'max_memory_usage_gb': 200}
        )
        
        logger.info("æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        cleanup_memory()
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        start_time = time.time()
        
        logger.info(f"å¼€å§‹è®­ç»ƒ Epoch {epoch}")
        
        for batch_idx, batch in enumerate(self.train_loader):
            try:
                # æ•°æ®ç§»åŠ¨åˆ°GPU
                if isinstance(batch, dict):
                    for key in batch:
                        if isinstance(batch[key], torch.Tensor):
                            batch[key] = batch[key].to(self.device, non_blocking=True)
                
                # å‰å‘ä¼ æ’­
                if self.scaler:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(batch)
                        loss = self.criterion(outputs, batch)
                else:
                    outputs = self.model(batch)
                    loss = self.criterion(outputs, batch)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                
                if self.scaler:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if batch_idx % 10 == 0:
                    cleanup_memory()
                    
                # è®°å½•è¿›åº¦
                if batch_idx % 50 == 0:
                    elapsed = time.time() - start_time
                    logger.info(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, "
                              f"Time: {elapsed:.1f}s")
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    logger.error(f"CUDAå†…å­˜ä¸è¶³ - Batch {batch_idx}")
                    cleanup_memory()
                    continue
                else:
                    raise e
        
        avg_loss = total_loss / max(num_batches, 1)
        epoch_time = time.time() - start_time
        
        logger.info(f"Epoch {epoch} å®Œæˆ - å¹³å‡æŸå¤±: {avg_loss:.4f}, ç”¨æ—¶: {epoch_time:.1f}s")
        
        return avg_loss, epoch_time
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                try:
                    if isinstance(batch, dict):
                        for key in batch:
                            if isinstance(batch[key], torch.Tensor):
                                batch[key] = batch[key].to(self.device, non_blocking=True)
                    
                    outputs = self.model(batch)
                    loss = self.criterion(outputs, batch)
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        cleanup_memory()
                        continue
                    else:
                        raise e
        
        avg_loss = total_loss / max(num_batches, 1)
        logger.info(f"éªŒè¯æŸå¤±: {avg_loss:.4f}")
        
        return avg_loss
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        
        best_val_loss = float('inf')
        
        for epoch in range(self.config['epochs']):
            try:
                # è®­ç»ƒ
                train_loss, epoch_time = self.train_epoch(epoch)
                
                # éªŒè¯
                val_loss = self.validate()
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    torch.save(self.model.state_dict(), 
                             '/nas/factor_forecasting/outputs/best_model.pth')
                    logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ - éªŒè¯æŸå¤±: {val_loss:.4f}")
                
                # è®°å½•epochå®Œæˆæ—¶é—´
                logger.info(f"=== Epoch {epoch} å®Œæˆ ===")
                logger.info(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
                logger.info(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
                logger.info(f"Epochç”¨æ—¶: {epoch_time:.1f}ç§’")
                logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                
                # å¼ºåˆ¶æ¸…ç†å†…å­˜
                cleanup_memory()
                
            except Exception as e:
                logger.error(f"Epoch {epoch} é”™è¯¯: {e}")
                cleanup_memory()
                continue

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("=" * 60)
        logger.info("ğŸš€ å¯åŠ¨ç»ˆæå†…å­˜å®‰å…¨è®­ç»ƒç³»ç»Ÿ")
        logger.info("=" * 60)
        
        # æ¸…ç†åˆå§‹å†…å­˜
        cleanup_memory()
        
        # åŠ è½½é…ç½®
        with open('/nas/factor_forecasting/memory_safe_config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        logger.info(f"é…ç½®åŠ è½½å®Œæˆ:")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        logger.info(f"  éšè—ç»´åº¦: {config['hidden_dim']}")
        logger.info(f"  å±‚æ•°: {config['num_layers']}")
        logger.info(f"  åºåˆ—é•¿åº¦: {config['sequence_length']}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MemorySafeTrainer(config)
        cleanup_memory()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        trainer.create_data_loaders()
        cleanup_memory()
        
        # åˆ›å»ºæ¨¡å‹
        trainer.create_model()
        cleanup_memory()
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        logger.info("âœ… è®­ç»ƒå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        cleanup_memory()
        raise
    finally:
        cleanup_memory()

if __name__ == "__main__":
    main()
